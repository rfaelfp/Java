<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Red Hat Summit Virtual Experience 2021: Register today</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qxEz7hH963I/" /><category term="Uncategorized" /><category term="Burr Sutter" /><category term="red hat summit" /><category term="Summit 2021" /><author><name>Red Hat Developer</name></author><id>https://developers.redhat.com/blog/?p=878357</id><updated>2021-03-08T08:00:55Z</updated><published>2021-03-08T08:00:55Z</published><content type="html">&lt;p&gt;Automation, application deployment, and how to speed up your journey to the cloud. These and other developer hot topics will take center stage at &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/summit"&gt;Red Hat Summit 2021&lt;/a&gt;. Join thousands of your peers by &lt;a target="_blank" rel="nofollow" href="https://reg.summit.redhat.com/flow/redhat/sum21/regGeneralAttendee/login?intcmp=7013a0000026UTXAA2"&gt;registering&lt;/a&gt; for our all-new, free, two-part virtual Summit experience. Keynote speaker Burr Sutter will be delving deep into developer technologies as we come together to learn, share stories of success and failure, and turn knowledge into action.&lt;/p&gt; &lt;p&gt;We’ve reimagined this year’s Red Hat Summit as a multi-part experience that includes two no-cost virtual components in April and June, followed by a series of small-scale in-person events later in the year.&lt;/p&gt; &lt;h2&gt;Virtual Experience | &lt;a target="_blank" rel="nofollow" href="https://www.addevent.com/event/gw5377211"&gt;April 27-28, 2021&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Join us online from wherever you are in the world.&lt;/p&gt; &lt;p&gt;What to expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keynotes from Red Hat leaders&lt;/li&gt; &lt;li&gt;Exciting news and announcements&lt;/li&gt; &lt;li&gt;Global customer and partner spotlights&lt;/li&gt; &lt;li&gt;Live demos&lt;/li&gt; &lt;li&gt;Access to Red Hat experts&lt;/li&gt; &lt;li&gt;Games and entertainment&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Virtual Experience | &lt;a target="_blank" rel="nofollow" href="https://www.addevent.com/event/Kx5377253"&gt;June 15-16, 2021&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Build on what you learned in April in this second installment.&lt;/p&gt; &lt;p&gt;What to expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seven channels of breakout sessions featuring in-depth technical content&lt;/li&gt; &lt;li&gt;Even more access to Red Hat experts&lt;/li&gt; &lt;li&gt;Customer stories and global content&lt;/li&gt; &lt;li&gt;Demos, chat lounges, and community engagement&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://reg.summit.redhat.com/flow/redhat/sum21/regGeneralAttendee/login?intcmp=7013a0000026UTXAA2"&gt;Visit the Red Hat Summit site to secure your spot at both events with one registration&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="/summit"&gt;Bookmark this page&lt;/a&gt; for the latest Summit-related developer sessions and on-demand videos.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#038;title=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/" data-a2a-title="Red Hat Summit Virtual Experience 2021: Register today"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/"&gt;Red Hat Summit Virtual Experience 2021: Register today&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qxEz7hH963I" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Automation, application deployment, and how to speed up your journey to the cloud. These and other developer hot topics will take center stage at Red Hat Summit 2021. Join thousands of your peers by registering for our all-new, free, two-part virtual Summit experience. Keynote speaker Burr Sutter will be delving deep into developer technologies as [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/"&gt;Red Hat Summit Virtual Experience 2021: Register today&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">878357</post-id><dc:creator>Red Hat Developer</dc:creator><dc:date>2021-03-08T08:00:55Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/</feedburner:origLink></entry><entry><title>New developer quick starts and more in the Red Hat OpenShift 4.7 web console</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ihpK80S_v6g/" /><category term="CI/CD" /><category term="Kubernetes" /><category term="Operator" /><category term="Serverless" /><category term="helm charts" /><category term="openshift" /><category term="serverless" /><category term="Tekton" /><author><name>Serena Chechile Nichols</name></author><id>https://developers.redhat.com/blog/?p=873037</id><updated>2021-03-08T08:00:26Z</updated><published>2021-03-08T08:00:26Z</published><content type="html">&lt;p&gt;We are continuing to evolve the developer experience in &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com"&gt;Red Hat OpenShift 4.7&lt;/a&gt;. This article highlights what&amp;#8217;s new for developers in the OpenShift 4.7 web console. Keep reading to learn about exciting changes to the topology view, an improved developer catalog experience, new developer quick starts, user interface support for &lt;a target="_blank" rel="nofollow" href="/courses/middleware/openshift-pipelines"&gt;Red Hat OpenShift Pipelines&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, and more.&lt;/p&gt; &lt;h2&gt;Quick-add in the topology view&lt;/h2&gt; &lt;p&gt;One of my favorite features in OpenShift 4.7 is the new quick-add option in the web console&amp;#8217;s topology view. You can use this UI control to search for an item from the developer catalog directly from the topology view without changing context. As you type, matches are dynamically shown in a list. You can then click on a match to see a quick overview in the right panel, then click on the call-to-action to install it. The demonstration in Figure 1 shows the new quick-add feature.&lt;/p&gt; &lt;div id="attachment_877827" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/quickadd.gif"&gt;&lt;img aria-describedby="caption-attachment-877827" class="wp-image-877827 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4.7-fig1.gif" alt="An animated demonstration of the quick-add feature." width="640" height="348" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877827" class="wp-caption-text"&gt;Figure 1: The new quick-add feature in the OpenShift web console&amp;#8217;s topology view.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Additionally, the web console now offers persistent storage for user settings so that you can persist layouts in the topology view. We&amp;#8217;ve had many requests for this feature, shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_877837" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/topology-layout.gif"&gt;&lt;img aria-describedby="caption-attachment-877837" class="wp-image-877837 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4-7-fig2.gif" alt="A demonstration of persistence in the topology graph layouts." width="640" height="369" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877837" class="wp-caption-text"&gt;Figure 2: Topology graph layouts are now persisted.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;New features in the developer catalog&lt;/h2&gt; &lt;p&gt;The developer catalog is a one-stop-shop for developers to get started quickly with OpenShift. We have improved the developer experience in OpenShift 4.7 by creating a consistent experience across catalogs while also offering contextual views for specific catalog types.&lt;/p&gt; &lt;p&gt;When entering the developer catalog, users can now view all content in a single catalog. Several sub-catalogs are available by default: Builder images, Helm charts, Operator-backed services, and samples. Other sub-catalogs are available based on the installed &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/operators"&gt;Operators&lt;/a&gt;. OpenShift 4.7 has sub-catalogs for event sources and virtual machines, and more are coming in future releases.&lt;/p&gt; &lt;p&gt;When you drill into sub-catalogs, the features and filters exposed are specific to a given catalog type. As an example, did you know that administrators can add multiple Helm chart repositories? The Helm chart catalog exposes charts from multiple repositories and lets you filter by any Helm chart repository.&lt;/p&gt; &lt;p&gt;Finally, we have received many requests to allow administrators to customize the developer catalog experience. In OpenShift 4.7, we&amp;#8217;ve added a customization feature for catalog administrators. To modify the developer catalog&amp;#8217;s available categories, you only need to add a customization section to the console operator resource. You can then use the resulting YAML snippet to add the default categories to start with and edit them from there.&lt;/p&gt; &lt;h2&gt;Developer quick starts&lt;/h2&gt; &lt;p&gt;You can now access developer quick starts from the &lt;b&gt;+Add&lt;/b&gt; page or from the &lt;b&gt;Quick Starts&lt;/b&gt; item in the OpenShift web console&amp;#8217;s &lt;b&gt;Help&lt;/b&gt; menu. The quick-starts catalog, shown in Figure 3, offers a variety of new developer quick starts—try one out!&lt;/p&gt; &lt;div id="attachment_873397" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev.png"&gt;&lt;img aria-describedby="caption-attachment-873397" class="wp-image-873397 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-1024x562.png" alt="Tiles represent quick starts in the catalog." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-1024x562.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-768x421.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-873397" class="wp-caption-text"&gt;Figure 3: Developer quick starts in the quick-starts catalog.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Tekton pipelines&lt;/h2&gt; &lt;p&gt;The OpenShift 4.7 web console offers a couple of enhancements for Tekton pipelines. For one, you can now easily access your Tekton pipeline metrics, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_873377" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics.png"&gt;&lt;img aria-describedby="caption-attachment-873377" class="wp-image-873377 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-1024x615.png" alt="A demonstration of viewing Tekton metrics in the console." width="640" height="384" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-1024x615.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-300x180.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-768x461.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-873377" class="wp-caption-text"&gt;Figure 4: Tekton pipeline metrics.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We&amp;#8217;ve also enhanced the &lt;b&gt;PipelineRun&lt;/b&gt; details page, as demonstrated in Figure 5.&lt;/p&gt; &lt;div id="attachment_877857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PLREnhancements.gif"&gt;&lt;img aria-describedby="caption-attachment-877857" class="wp-image-877857 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4-7-fig5.gif" alt="A demonstration of viewing the PipelineRun details page." width="640" height="369" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877857" class="wp-caption-text"&gt;Figure 5: The improved PipelineRun details page.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;From the &lt;b&gt;Events&lt;/b&gt; tab, you can now easily access events related to the &lt;code&gt;PipelineRun&lt;/code&gt;, including &lt;code&gt;TaskRun&lt;/code&gt; and pod events. You can also download logs from the &lt;b&gt;Logs&lt;/b&gt; tab.&lt;/p&gt; &lt;h2&gt;Serverless&lt;/h2&gt; &lt;p&gt;Web console support for OpenShift Serverless includes the ability to create channels. Once created, brokers and channels are displayed in the topology view. In addition to creating subscriptions and triggers from action menus, you can now drag-and-drop to initiate these actions from the topology view.&lt;/p&gt; &lt;p&gt;We have also enhanced the creation flow for event sources. Event sources are custom resources, and we needed to address scalability issues in this feature, so we’ve changed the user experience to be catalog-based. You can now view event sources along with other objects in the service catalog. Alternatively, you can click on the event source type and drill into a catalog solely focused on event sources. As an example, if you had the &lt;a target="_blank" rel="nofollow" href="/integration"&gt;Red Hat Integration&lt;/a&gt; &lt;a target="_blank" rel="nofollow" href="/topics/camel-k"&gt;Camel K&lt;/a&gt; Operator installed, you would see Camel K connectors in the catalog.&lt;/p&gt; &lt;p&gt;We’ve also updated the administrator perspective for OpenShift Serverless. The web console includes a primary navigation section for OpenShift Serverless, which contains two sub-sections. One sub-section focuses on serving resources, and the other is for eventing. You can navigate to these sections to find details about your OpenShift event sources, brokers, triggers, channels, and subscriptions. These items are also accessible in the developer perspective&amp;#8217;s topology view and on the search page.&lt;/p&gt; &lt;h2&gt;We want your feedback&lt;/h2&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience, and we want to hear from you. You can attend our office hours on &lt;a target="_blank" rel="nofollow" href="http://openshift.tv"&gt;Red Hat OpenShift Streaming&lt;/a&gt; or join the &lt;a target="_blank" rel="nofollow" href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google group&lt;/a&gt;. We hope you will share your tips for using the OpenShift web console, get help with what doesn’t work, and shape the future of the OpenShift developer experience. Ready to get started? &lt;a target="_blank" rel="nofollow" href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#038;title=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/" data-a2a-title="New developer quick starts and more in the Red Hat OpenShift 4.7 web console"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/"&gt;New developer quick starts and more in the Red Hat OpenShift 4.7 web console&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ihpK80S_v6g" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;We are continuing to evolve the developer experience in Red Hat OpenShift 4.7. This article highlights what&amp;#8217;s new for developers in the OpenShift 4.7 web console. Keep reading to learn about exciting changes to the topology view, an improved developer catalog experience, new developer quick starts, user interface support for Red Hat OpenShift Pipelines and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/"&gt;New developer quick starts and more in the Red Hat OpenShift 4.7 web console&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">873037</post-id><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2021-03-08T08:00:26Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/</feedburner:origLink></entry><entry><title>What’s new in Red Hat OpenShift’s Web Terminal Operator 1.2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KxWHVnxmaAk/" /><category term="DevOps" /><category term="Kubernetes" /><category term="Linux" /><category term="Operator" /><category term="openshift" /><category term="OpenShift Operator" /><category term="web console" /><category term="Web Terminal Operator" /><author><name>jpinkney</name></author><id>https://developers.redhat.com/blog/?p=872357</id><updated>2021-03-08T08:00:11Z</updated><published>2021-03-08T08:00:11Z</published><content type="html">&lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&amp;#8216;s Web Terminal Operator is a way for users to access a web terminal with common cluster tooling pre-installed. This gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally.&lt;/p&gt; &lt;p&gt;This article is an overview of the new features introduced in Web Terminal Operator 1.2. These improvements include allowing cluster administrators to securely access the terminal, more information for users when a terminal has shut down due to inactivity, and a tooling update to align with OpenShift 4.7.&lt;/p&gt; &lt;h2&gt;Easier access to the OpenShift web console&lt;/h2&gt; &lt;p&gt;In Web Terminal Operator 1.2, cluster administrators can access the web terminal directly, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_872407" style="width: 650px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-1.gif"&gt;&lt;img aria-describedby="caption-attachment-872407" class="wp-image-872407" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-1.gif" alt="A cluster administrator accessing the web terminal on the OpenShift web console." width="640" height="332" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872407" class="wp-caption-text"&gt;Figure 1: Opening the web terminal as a cluster administrator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, everyone on your cluster can access their own web terminal, regardless of their permission level. Note that for security reasons, cluster administrators will automatically bypass the namespace picker and have their web terminal created in the &lt;code&gt;openshift-terminal&lt;/code&gt; namespace. Other than that, the functionality remains the same as that of a user without the cluster-admin role.&lt;/p&gt; &lt;h2&gt;Understanding why a terminal has stopped&lt;/h2&gt; &lt;p&gt;To conserve resources, the web terminal automatically shuts down after 15 minutes of inactivity. In Web Terminal Operator 1.2, we’ve added more information to help users understand why a terminal has stopped (see Figure 2).&lt;/p&gt; &lt;div id="attachment_872417" style="width: 650px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png"&gt;&lt;img aria-describedby="caption-attachment-872417" class="wp-image-872417" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png" alt="The web console informing the user that the terminal has closed due to inactivity." width="640" height="333" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png 821w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2-300x156.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2-768x399.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872417" class="wp-caption-text"&gt;Figure 2: The terminal window indicating why a terminal has shut down.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Web Terminal Operator 1.2 also offers an easier way to restart the terminal with the click of a button, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_872427" style="width: 649px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-3.gif"&gt;&lt;img aria-describedby="caption-attachment-872427" class="wp-image-872427" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-3.gif" alt="Clicking the Restart terminal button to restart the session." width="639" height="336" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872427" class="wp-caption-text"&gt;Figure 3: Click &lt;b&gt;Restart terminal&lt;/b&gt; to restart a session.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Updated tooling&lt;/h2&gt; &lt;p&gt;We have updated the default binaries in Web Terminal Operator 1.2 to include the latest versions of the built-in command-line tools, as shown in Table 1.&lt;/p&gt; &lt;table align="”center”"&gt; &lt;caption&gt;&lt;b&gt;Table 1: Command-line tools in Web Terminal Operator 1.2&lt;/b&gt;&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Binary&lt;/th&gt; &lt;th&gt;Old version&lt;/th&gt; &lt;th&gt;New version&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;oc&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4.6.1&lt;/td&gt; &lt;td&gt;4.7.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1.19.0&lt;/td&gt; &lt;td&gt;1.20.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;odo&lt;/code&gt;&lt;/td&gt; &lt;td&gt;2.0.0&lt;/td&gt; &lt;td&gt;2.0.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;helm&lt;/code&gt;&lt;/td&gt; &lt;td&gt;3.3.4&lt;/td&gt; &lt;td&gt;3.5.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;kn&amp;#60;/code&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.16.1&lt;/td&gt; &lt;td&gt;0.19.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;tkn&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.11.0&lt;/td&gt; &lt;td&gt;0.15.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;For a peek into how the Web Terminal Operator works under the hood, see &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;&lt;i&gt;A deeper look at the Web Terminal Operator&lt;/i&gt;&lt;/a&gt; by Angel Misevski. You can also check out the initial release article by Joshua Wood: &lt;a target="_blank" rel="nofollow" href="/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/"&gt;&lt;i&gt;Command-line cluster management with Red Hat OpenShift’s new web terminal&lt;/i&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#038;title=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/" data-a2a-title="What’s new in Red Hat OpenShift’s Web Terminal Operator 1.2"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/"&gt;What&amp;#8217;s new in Red Hat OpenShift&amp;#8217;s Web Terminal Operator 1.2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KxWHVnxmaAk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;&amp;#160; Red Hat OpenShift&amp;#8216;s Web Terminal Operator is a way for users to access a web terminal with common cluster tooling pre-installed. This gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally. This article is an overview [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/"&gt;What&amp;#8217;s new in Red Hat OpenShift&amp;#8217;s Web Terminal Operator 1.2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">872357</post-id><dc:creator>jpinkney</dc:creator><dc:date>2021-03-08T08:00:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 1: Overview</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pilWCtxrdN0/" /><category term="JavaScript" /><category term="Node.js" /><category term="Open source" /><category term="npm" /><category term="reference architecture" /><author><name>Michael Dawson</name></author><id>https://developers.redhat.com/blog/?p=865807</id><updated>2021-03-08T08:00:06Z</updated><published>2021-03-08T08:00:06Z</published><content type="html">&lt;p&gt;Welcome to this new series introducing the &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt; from Red Hat and IBM. This article is an overview of our reasons for developing the Node.js reference architecture—both what we hope the architecture will offer our developer community and what we &lt;em&gt;do not&lt;/em&gt; intend it to do. Future articles will offer a detailed look at different sections of the reference architecture.&lt;/p&gt; &lt;p&gt;Before we dive into this first article, it&amp;#8217;s important to acknowledge that the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; reference architecture is a work in progress. The development team is working through different areas, discussing what we&amp;#8217;ve learned, and distilling that information into concise recommendations and guidance. Given the fast pace of development in the &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; ecosystem, the reference architecture might never be “finished.” Instead, we&amp;#8217;ll continue updating it to reflect what we learn through new Node.js production deployments and ongoing experience with our deployments at scale. The reference architecture is meant to reflect our current experience and thinking, which will evolve.&lt;/p&gt; &lt;h2&gt;Why we need a Node.js reference architecture&lt;/h2&gt; &lt;p&gt;The JavaScript ecosystem is fast-moving and vibrant. You only need to look at the &lt;a target="_blank" rel="nofollow" href="http://www.modulecounts.com/"&gt;growth rate of Node Package Manager (npm) modules&lt;/a&gt; to see that. In 2016, there were approximately 250,000 npm packages. In 2018, that number climbed to around 525,000, and in 2020 it was roughly 1.1 million. These numbers represent considerable choice and variety in the JavaScript ecosystem. That is clearly a strength for flourishing innovation and testing new ideas.&lt;/p&gt; &lt;p&gt;On the flip side, the wide variety of options can make choosing among Node.js packages very difficult. For any module, you might find several equally good choices, as well as several potentially very bad choices. Every application has a “secret sauce” that is key to its success. It is imperative to find the best fitting, newest, or most innovative package to use for this area of the application. For the rest of the application, you likely want something that works and for which you can share any experiences or best practices across your organization. In the latter case, having a reference architecture can help teams avoid relearning the same things again and again.&lt;/p&gt; &lt;h2&gt;What the reference architecture is&lt;/h2&gt; &lt;p&gt;Our Node.js teams at Red Hat and IBM can&amp;#8217;t be experts on 1.1 million JavaScript packages in the &lt;code&gt;npm&lt;/code&gt; registry. Similarly, we can&amp;#8217;t be involved in all of the projects to the level that we are involved in the Node.js project. Instead, our experience is based on our broad usage of Node.js. This includes large-scale deployments like the &lt;a target="_blank" rel="nofollow" href="https://developer.ibm.com/languages/node-js/articles/nodejs-weather-company-success-story/"&gt;Weather Company&lt;/a&gt;, as well as the work that our consulting groups do with customers.&lt;/p&gt; &lt;p&gt;If every internal team and customer who asks for help with their Node.js application uses different packages, it will be much harder to help them. The question is, how do we share our knowledge across the organization?&lt;/p&gt; &lt;p&gt;We want to help our internal teams and customers make good choices and deployment decisions. In cases where a team doesn&amp;#8217;t need to use a specific package, we can recommend a package based on the experience we’ve built across Red Hat and IBM. As developers, we can use the Node.js reference architecture to share and collaborate across teams and projects and establish common ground within our deployments.&lt;/p&gt; &lt;h2&gt;What the reference architecture is not&lt;/h2&gt; &lt;p&gt;I have described what we hope to do with the Node.js reference architecture. It is just as important to be clear about what we are &lt;em&gt;not&lt;/em&gt; trying to do.&lt;/p&gt; &lt;p&gt;First, the reference architecture is not an attempt to convince or force developers to use the packages we choose. Deployments are varied, and there will be good reasons to use specific modules in different circumstances.&lt;/p&gt; &lt;p&gt;Second, we do not claim that our recommendations are better than the alternatives. As I noted, you will often find several equally good packages or approaches available in the JavaScript ecosystem. Our recommendations favor what the Red Hat and IBM teams have used successfully and the technologies we are familiar with. We are not attempting to steer anyone to the “best” choice but instead to a “good” choice. Having a reference architecture maximizes the likelihood of leveraging lessons already learned and having common ground so that we can help each other.&lt;/p&gt; &lt;h2&gt;About this series&lt;/h2&gt; &lt;p&gt;The Node.js development team is having interesting discussions as we work through each section of the reference architecture. At the same time, we are trying to keep the reference architecture&amp;#8217;s content concise and to the point. As I&amp;#8217;ve mentioned, the goal is to provide good choices for the application&amp;#8217;s general architecture so that developers can focus on the application&amp;#8217;s &amp;#8220;secret sauce.&amp;#8221; In most cases, developers using the reference architecture will want to know what package or technology to use and how. As a result, the reference architecture won&amp;#8217;t include much about the interesting background and discussions that led to our decisions.&lt;/p&gt; &lt;p&gt;This series &lt;em&gt;will&lt;/em&gt; share the viewpoints gained from our internal discussions. As we work through each section of the reference architecture, we&amp;#8217;ll use this series to offer additional references and an opportunity to dive into more detail on related topics. I think you’ll find the varied experience of developers across the Node.js team gets you thinking. I learn something from every section we go through, and I hope you will, too.&lt;/p&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of this series. While you wait for the next installment, we invite you to visit the &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub. You&amp;#8217;ll be able to see the work we&amp;#8217;ve already done and the kinds of topics you can look forward to from this series. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#038;title=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/" data-a2a-title="Introduction to the Node.js reference architecture, Part 1: Overview"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Introduction to the Node.js reference architecture, Part 1: Overview&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pilWCtxrdN0" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Welcome to this new series introducing the Node.js reference architecture from Red Hat and IBM. This article is an overview of our reasons for developing the Node.js reference architecture—both what we hope the architecture will offer our developer community and what we do not intend it to do. Future articles will offer a detailed look [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Introduction to the Node.js reference architecture, Part 1: Overview&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">865807</post-id><dc:creator>Michael Dawson</dc:creator><dc:date>2021-03-08T08:00:06Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/</feedburner:origLink></entry><entry><title type="html">Supply chain integration - An architectural introduction</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RglmmpBvDao/supply-chain-integration-an-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/pvxTT3ZYk8A/supply-chain-integration-an-architectural-introduction.html</id><updated>2021-03-08T06:00:00Z</updated><content type="html">Part 1 - An architectural introduction If you've been following my writing over the last few years, you've grown accustomed to my sharing of various architecture blueprints. They're focusing on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in our mission of creating of architectural content based on common customer adoption patterns. That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Architectural Blueprint.  Let's look at these blueprints, how their created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division in to smaller parts. In this case we've aligned with the higher level theme being 'Retail' use cases, a vertical focus. This breaks down into the following use cases and in no particular order: * * * Point of sale * Headless eCommerce * Store health and safety * Real-time stock control * Retail data framework The case I'm tackling here is focused on supply chain integration. This use case we've defined as the following: Streamlining integration between different elements of a retail supply chain for on-premise, cloud, and other third-party interactions. The approach taken is to research our existing customers that have implemented solutions in this space, collect their public facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  To get an idea of what these blueprints look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architectural blueprint for use telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on business optimisation portfolio architecture blueprint: 1. 2. Common architectural elements 3. Example of supply chain integration Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at the generic common architectural elements for the supply chain integration architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RglmmpBvDao" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/pvxTT3ZYk8A/supply-chain-integration-an-architectural-introduction.html</feedburner:origLink></entry><entry><title type="html">Trying DB2 in Kubernetes for developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qPr-LeKzOYU/" /><author><name /></author><id>https://blog.ramon-gordillo.dev/2021/03/trying-db2-in-kubernetes-for-developers/</id><updated>2021-03-08T00:00:00Z</updated><content type="html">Recently I have been asked a lot of similar questions like “does it make sense to invest in deploying legacy technology on kubernetes?". Well, first of all, I have to say that some technologies that are considered “legacy” have tons of new exciting features for the new workloads and applications. This is, for example, the case of DB2 with spatial queries and many others that you may have a look at (to be honest, I have very few experience with it). Secondly, not all the applications are greenfield. There are lots of old ones that are using those “legacy” technologies. Even some greenfield ones need to use data stored in long life systems. If I want to modernize those applications or build new ones based on these data, I should be able to develop in a friendly environment. And as a developer, I would be more than happy if I can create an instance where I have total control and that I can reuse the agility of containers. That is why I was tempted to try DB2 on containers. I was looking at an alternative to do as quickly as possible, so that is why I took some decisions that maybe do not follow exactly the guidelines that I would link as a reference. DISCLAIMER This is just a personal exercise, it does not represent IBM procedures or recommendations in any way. DB2® is copyright by IBM, every time I refer to DB2 it should be assumed implicitly DB2®. It is not an open source technology under different licences by IBM, in this case I would use the community edition, whose license and other details are commented . I would like to mention that is only an exercise. Please, use the recommended, documented and supported procedure if you need an stable version for your particular use case. Additionally we should remember db2 technology as most of the traditional databases, uses low level access to some resources (like shared memory, processes, and so on) to perform best on bare metal/virtual environments. That means it still , and although they are working on adapting better to cloud and containers, we are not used to see them in container native technologies. PREREQUISITES KUBERNETES As I have done in previous posts, I am using minikube for this exercise and a local domain served by dnsmasq on my laptop. That provides me the ability of working offline once the container images are deployed on the cluster. In my test I have used a configuration or 6 cpus and 12 Gb of RAM, as suggested in . If we are not deploying the Unified Console, we can reduce it to 4 cpus. I used the following command (check your driver): minikube start --cpus=6 --memory=12288 --driver=kvm2 --addons dashboard,ingress,metrics-server Additionally, in some steps we will need some ingress objects with passthrough ssl. That will simplify to provide SNI negotiation. To , review the deployment and add the --enable-ssl-passthrough arguments to the container ones. IBM API KEY You should register in IBM as a developer, and create an API key. I have used the console using . DEPLOYMENT I have modified a couple of things from the documentation I have found and linked, the most relevant are: * Use helm 3 instead of helm 2, because after November 2020 . * In vanilla kubernetes for developers, we don’t need to worry about , which provides an extra security to OpenShift environments. As our dev cluster is in a laptop, I can relax a little bit the security requirements (note: if the environment is shared, review the security or consider alternatives to avoid bad times). PREPARATION First, clone the repo . Go to stable/ibm-db2 folder, where there you can find most of the artifacts and information to deploy db2 community edition on OpenShift. We are now creating the namespace for deploying the database. We are creating a service account and provide the privileges through a role and a role binding to create some additional objects, as the helm chart will deploy some jobs to create the whole infrastructure on the namespace. kubectl create namespace db2 kubectl create serviceaccount db2u -n db2 kubectl create -f ibm_cloud_pak/pak_extensions/pre-install/namespaceAdministration/ibm-db2-role.yaml -n db2 kubectl create -f ibm_cloud_pak/pak_extensions/pre-install/namespaceAdministration/ibm-db2-rb.yaml -n db2 Then, as we will use a private registry (the ibm one), we need to set up the credentials for it. With the API Key that you should have obtained in the prerequisites, create a secret and add it to the service account to allow pulling the images needed for the deployment. kubectl create secret -n db2 docker-registry ibm-registry \ --docker-server=icr.io \ --docker-username=iamapikey \ --docker-password=&lt;api_key&gt; kubectl patch serviceaccount db2u -n db2 -p '{"imagePullSecrets": [{"name": "ibm-registry"}]}' We are now ready to deploy the database. DEPLOYMENT The official deployment script is db2u-install on the ibm_cloud_pak/pak_extensions/common folder. It encapsulates the helm 2 script, so I have done a little hack in order to use helm 3 and avoid using tiller. Also, I wanted to get the secrets created automatically, so I need to change generateSecrets=true that is false in the original script. With all those changes, we can create the jobs that will deploy all the infra, executing the helm command from stable/ibm-db2 folder. helm install db2poc $PWD --namespace db2 --values $PWD/values.yaml --set arch=x86_64 --set servicename=db2poc --set ldap.ldap_server=db2poc-db2u-ldap --set global.dbType=db2oltp --set database.name=BLUDB --set generateSecrets=true --set storage.storageClassName=standard --set storage.useDynamicProvisioning=true And voilá, after downloading images, and creating everything,we have a brand new db2 instance in our kuberenets cluster. We are going to do some additional tests to double check everything is ok. In the following picture, we can see the main architecture for the deployment. CREATING A DEVELOPER USER From , we have some hints on how to create an user on the ldap that this db2 instance uses as identity provider. tools_pod=$(kubectl get po -n db2 -o name | grep db2poc-db2u-tools) kubectl exec -it ${tools_pod} -- addLdapUser.py -u admin -p h4ck1t -r admin We will get something like Next UID will be 5003 Adding admin to LDAP server Updating LDAP password for user admin Added user to LDAP server After creating the user on the ldap, we check it is valid for use as db2 user: kubectl exec -it db2poc-db2u-0 -- id admin kubectl exec -it db2poc-db2u-0 -- su db2inst1 -c "~/sqllib/bin/db2 connect to bludb user admin using h4ck1t" If everything is ok, we will get uid=5003(admin) gid=3000(bluadmin) groups=3000(bluadmin) Database Connection Information Database server = DB2/LINUXX8664 11.5.4.0 SQL authorization ID = ADMIN Local database alias = BLUDB OPTIONAL: UNIFIED CONSOLE The instructions to deploy the unified console in the same namespace as the db2 instance are explaine in . Like previously, we prefer to use helm 3, so instead of using deploy-console.sh script, we are running directly the helm command. From the root path of your github clone, move to the stable/ibm-unified-console folder. Then, run the following command. helm install db2poc-console $PWD --set configMapName=db2poc-db2u-uc-config --set dataServer.ldap.rootPwdSecretName=db2poc-db2u-ldap --set dataServer.metadb.pwdSecretName=db2poc-db2u-instance --set dataServer.sharedPVC.name=db2poc-db2u-sqllib-shared --set global.image.secretName=ibm-registry -f $PWD/ibm_cloud_pak/pak_extensions/values-standalone-console.yaml --namespace db2 After deploying the chart, an additional ingress is needed. The deploy-console.sh creates the route object for OpenShift, a similar command for vanilla kubernetes ingress on nginx whould be: cat &lt;&lt; _EOF_ |kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: db2poc-console namespace: db2 annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: "true" spec: rules: - host: db2poc-console.db2.minikube.cloud http: paths: - backend: service: name: db2poc-console-ibm-unified-console-ui port: number: 443 path: / pathType: ImplementationSpecific _EOF_ If everything is right, you may login into the ingress host (in my case, ) with the user credentials you have created previously and see a picture like the following. The whole architecture including the unified console now is as depicted. TESTING The chart deploys a nodeport service to access the database connecting through plain and ssl ports. We are going to do some initial tests based on it, but we are going to set up an ingress to use ssl passthrough to connect from outside the cluster. I am testing the connection using , which is under . These instructions will be similar to other jdbc-based database tools. At the time of writing this post, I have used the 11.5.4 client jdbc driver that can be downloaded from . As a reference, I use the document for setting up the connection. echo "Minikube IP is" $(minikube ip) echo "Non-secure NodePort is" $(kubectl get service db2poc-db2u-engn-svc -o jsonpath='{.spec.ports[?(@.name=="legacy-server")].nodePort}' ) Bearing in mind the default database is bludb, we add the alias jdbc:db2://&lt;minikube_ip&gt;:&lt;nodeport&gt;/bludb to connect, and get a successful session through the unsecure nodeport. I usually prefer to connect through a ssl passthrough ingress to the database from outside. A nodeport is ok, but has some drawbacks. If you want to try the ingress option, it is as simple as creating it with: cat &lt;&lt; _EOF_ |kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: db2poc-db2u namespace: db2 annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: "true" spec: rules: - host: "db2poc-db2u.minikube.cloud" http: paths: - backend: service: name: db2poc-db2u port: number: 50001 path: / pathType: ImplementationSpecific _EOF_ Now, you need the cert chain to add to the jdbc properties. You can extract the CA and the certificate connecting with openssl to the secured ingress connection, in my case using the following. echo -n | openssl s_client -connect db2poc-db2u.minikube.cloud:443 -servername db2poc-db2u.minikube.cloud -showcerts 2&gt;/dev/null |sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; /tmp/certs.pem In order to double check, the cert subject should be: CN=ibm.com,OU=ICP,O=IBM,L=Toronto,ST=Ontario,C=CA CN=ibm.com,OU=db2ssl,O=IBM,L=Toronto,ST=Ontario,C=CA You need to add those certs to a java keystore (jks). We can do it through the command line with keytool or use a visual tool like . Once you have the jks, you only need to add that information to the jdbc. A brief and useful can help you through the parameters. An example of secured url through the ingress is jdbc:db2://db2poc-db2u.minikube.cloud:443/bludb:sslConnection=true;sslTrustStoreLocation=/tmp/db2-server.jks;sslTrustStorePassword=h4ck1t;. Don’t forget the semicolon at the end of the url! MORE INFORMATION Other than the previous links and their related pages, you may find some useful information about the architecture, sizing, etc, for a production deployment on OpenShift in&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qPr-LeKzOYU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://blog.ramon-gordillo.dev/2021/03/trying-db2-in-kubernetes-for-developers/</feedburner:origLink></entry><entry><title type="html">Starting business processes using Kafka events</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lOnWfH1OUk8/starting-business-processes-using-kafka-events.html" /><author><name>Karina Varela</name></author><id>https://blog.kie.org/2021/03/starting-business-processes-using-kafka-events.html</id><updated>2021-03-07T23:30:57Z</updated><content type="html">Let’s check how we can model a business process, using the BPMN standard, that can react to events. Whenever a new event is published in a specific Kafka topic, a new process instance should be started. We’ll also check how to configure the project and environment in order to achieve these goals. GETTING STARTED To get started we should create, deploy and test an event-driven process application.  1. PREPARING YOUR ENVIRONMENT The samples described in this guide were created using the following technologies: * Java 11 * Maven, Git *  7.48+ or  7.10+ * Kafka  INFO: This feature was released in this specific jBPM and RHPAM version. To achieve this post’s goals, you must use the mentioned versions or higher. If you don’t know how to install jBPM locally, take a look at: . 1.1. PREPARING YOUR KAFKA SERVER AND TOPICS Event-driven processes interacts with other services via event platforms, more specifically in our case, Kafka topics. In this application, our process needs interacts with three topics: “incoming-requests“, “requests-approved” and “requests-denied“. Let’s now setup a Kafka environment and create these three topics. We will use Strimzi and docker compose to help us get up and running faster. INFO: This guide focus is not Kafka, therefore the following steps are straightforward. If you need more details about the upcoming commands please refer to this post: . First, clone the project that contains the docker-compose file we’ll use to start the Kafka services. Next start the services. Check the commands below: git clone cd amq-examples/strimzi-all-in-one/ docker-compose up Open a new tab in your terminal, access the cloned project folder (amq-examples/strimzi-all-in-one/) and create the three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied Now that we have the three topics ready to be used as our communication layer between services and our process, we can start working on the process definition. THE USE CASE AND THE SHIFT TO AN EVENT-DRIVEN PROCESS APPLICATION Our use case will be the automation of a credit card limit raise approval process. Most card issuers allow customers to request an increased credit limit through their websites, mobile apps or over the phone. Let’s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. TIP: We’ll simplify the business logic of this use case to give focus to the eventing features and how you can use it. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual analysis. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this guide’s scenario). Finally, the process ends either with an approved or denied request. Image 1: Process v1. The process starts based on a rest call or JAVA API invocation. Services involved in the process are invoked via rest. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic “request-approved” in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Notice how the process below can achieve the same goals, using an event-driven strategy: Image 2: Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status. In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order. 2. ENABLING THE JBPM (RHPAM) KAFKA EXTENSION To enable Kafka capabilities in the KIE Server (engine) we need to use system properties in the runtime environment. You can enable it both for SpringBoot and WildFly (a.k.a. jBoss) based deployments. See below the command that uses the jboss-cli.sh (or .bat) script to add the system property in WildFly, and, restart it. TIP: When adding new system properties to WildFly or jBoss EAP, it’s necessary to restart it to have the new system properties activated. INFO: There are more options in jBPM to customize the Kafka address, topic names, etc. In our case, we’re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: . With WildFly or EAP up and running, you can enable the Kafka extension in the KIE Server by executing the commands below: $ $JBOSS_HOME/bin/jboss-cli.sh -c [standalone@localhost:9990 /] /system-property=org.kie.kafka.server.ext.disabled:add(value=false) [standalone@localhost:9990 /] :shutdown(restart=true) We’re now ready to start working on the process definition. 3. STARTING PROCESSES USING EVENTS * First, import the existing project with process v1 in Business Central. * Open the cc-limit-raise-approval process. * The first step is to change the start event to a start message event: Image 3: Convert start event to start message event Whenever a customer do a new request (independently of the channel used) an event should be published on the “new-requests” topic. With that, a new process instance will be started whenever a new event is published in this topic. * Configure the name of the Kafka topic in the starting message event. Image 4: Configure the message with the same name of the topic it will listen to. * We want to receive the request contained in the event data. The engine provides automatic marshalling to help us mapping the input directly to a data object. The project has an object named “LimitRaiseRequest.java” which we will use to receive the incoming data. On the properties panel of the Start Message Event, configure the input data: Image 5: Start message event configuration of the Input data * Save the process. * Now, deploy the project to KIE Server so you can test it. You can use the deploy button available in Business Central. * Open a new tab in the terminal, and access the “” project we’re using to interact with the Kafka service. cd $PROJECTS_DIR/amq-examples/strimzi-all-in-one docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 &gt; The producer service is now waiting for you to publish an event to the topic “incoming-requests“. To do so, simply input the following json data and hit enter: {"data" : {"customerId": 1, "customerScore": 250, "requestedValue":1500}} &gt; {"data" : {"customerId": 1, "customerScore": 250, "requestedValue":1500}} * Now, in your browser, in Business Central, if you go to the Process Instances management page and filter by the Completed status, you should be able to see a process instance completed: Image 6: Business Central. List of completed process instances in the monitored KIE Server. * Select the process instance you see, and next, go the the Diagram tab. You should see that the request was automatically approved. Image 7: Process Instance Diagram. This process instance was started based on an event that happened in the topic configured in the message start event. -------------------------------------------------------------------------------- You can now effectively handle the events that triggers business processes within an event-driven architecture. The next step is to learn how to emit events from within your process. The following post should bring you details on how to let the ecosystem know about key happenings of your business process. -------------------------------------------------------------------------------- This blog post is part of the seventh section of the  series: . The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lOnWfH1OUk8" height="1" width="1" alt=""/&gt;</content><dc:creator>Karina Varela</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/starting-business-processes-using-kafka-events.html</feedburner:origLink></entry><entry><title>Making environment variables accessible in front-end containers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bRQAOiM1CwA/" /><category term="Containers" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="angular" /><category term="environment variables" /><category term="front-end containers" /><category term="react" /><category term="VueJS" /><author><name>Joel Lord</name></author><id>https://developers.redhat.com/blog/?p=861157</id><updated>2021-03-04T18:59:36Z</updated><published>2021-03-04T18:59:36Z</published><content type="html">&lt;p&gt;When &lt;a href="https://developers.redhat.com/topics/containers"&gt;building a container&lt;/a&gt; for a single-page application using any modern &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript framework&lt;/a&gt; (such as Angular, React, or Vue.js), you might find that the configuration settings are different depending on where the container will run. A typical case would be the base URL for your API, which will differ depending on whether you are testing the application or deploying it into production. Developers usually solve this problem using environment variables.&lt;/p&gt; &lt;p&gt;Environment variables typically work on the backend because that is where code runs. But what if your application lives in the user&amp;#8217;s browser? There are many ways around this limitation. In some cases, you might build a server whose endpoint holds the necessary parameters. Another workaround is to use PHP to inject the environment variables as globals in the JavaScript code. Both of these options work, but it would be ideal to inject the environment variables as part of the container build process. That way, you don&amp;#8217;t have to change the codebase, and you can still deliver the application content using a static web server like NGINX.&lt;/p&gt; &lt;p&gt;This article shows you how to inject environment variables directly into your codebase as you build your container.&lt;/p&gt; &lt;h2&gt;JavaScript frameworks in the production build&lt;/h2&gt; &lt;p&gt;It doesn&amp;#8217;t matter which JavaScript framework you use—React, Angular, or Vue.js—because they all work virtually the same way. The framework runs a server that watches the files, and it refreshes the browser when a change is detected. This process is excellent for development purposes but not so much for production servers. All of that code requires too many resources to run. For the application content to work in a web server, we need a build step that minimizes the code and keeps only the necessary parts. We can then create a package using a single page that contains all of the application&amp;#8217;s HTML, JavaScript, and CSS. When a container runs in a production environment, it will serve this minified package.&lt;/p&gt; &lt;p&gt;It turns out that the container-build step that prepares your code for production is also a great place to inject environment variables. We&amp;#8217;ll go through the process in the next sections.&lt;/p&gt; &lt;h2&gt;Create a skeleton application&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with a skeleton application built with the command-line interface (CLI) for your JavaScript framework:&lt;/p&gt; &lt;pre&gt;# Angular npx @angular/cli new angular-project # React npx create-react-app react-project # VueJS npx @vue/cli create vue-project &lt;/pre&gt; &lt;p&gt;For your project of choice, create a &lt;code&gt;config.json&lt;/code&gt; file in the &lt;code&gt;/src&lt;/code&gt; folder. This file will contain settings that could change based on the environment. In this case, it will have two properties: One to specify the environment and another one for the base URL of your imaginary API:&lt;/p&gt; &lt;pre&gt;{ "ENV": "development", "BASE_URL": "http://localhost:3000" }&lt;/pre&gt; &lt;p&gt;For simplicity, the application you are using will display those values on the main page. Head over to your main page, import the configuration file, and display both values in that view.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll look at the application-specific code for Angular, React, and Vue.js.&lt;/p&gt; &lt;h3&gt;Angular&lt;/h3&gt; &lt;p&gt;To import a JSON file, you might need to add the following options to the &lt;code&gt;compilerOptions&lt;/code&gt; of your &lt;code&gt;tsconfig.json&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; "resolveJsonModule": true, "esModuleInterop": true, "allowSyntheticDefaultImports": true, &lt;/pre&gt; &lt;p&gt;Here are the application components (&lt;code&gt;src/app/app.component.ts&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;import { Component } from '@angular/core'; import Config from "../config.json"; @Component({ selector: 'app-root', templateUrl: './app.component.html' }) export class AppComponent { environment = Config.ENV; baseUrl = Config.BASE_URL; }&lt;/pre&gt; &lt;p&gt;Here is the application HTML (&lt;code&gt;src/app/app.component.html&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&amp;#60;div&amp;#62; &amp;#60;p&amp;#62;Environment: {{ environment }}&amp;#60;/p&amp;#62; &amp;#60;p&amp;#62;Base Url: {{ baseUrl }}&amp;#60;/p&amp;#62; &amp;#60;/div&amp;#62; &lt;/pre&gt; &lt;h3&gt;React&lt;/h3&gt; &lt;p&gt;Here&amp;#8217;s an application config for React (&lt;code&gt;src/App.js&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;import Config from "./config.json"; function App() { const environment = Config.ENV; const baseUrl = Config.BASE_URL; return ( &amp;#60;div&amp;#62; &amp;#60;p&amp;#62;Environment: { environment }&amp;#60;/p&amp;#62; &amp;#60;p&amp;#62;Base Url: { baseUrl }&amp;#60;/p&amp;#62; &amp;#60;/div&amp;#62; ); } export default App;&lt;/pre&gt; &lt;h3&gt;Vue.js&lt;/h3&gt; &lt;p&gt;And here&amp;#8217;s the configuration for Vue.js (&lt;code&gt;src/App.vue&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&amp;#60;template&amp;#62; &amp;#60;div&amp;#62; &amp;#60;p&amp;#62;Environment: {{ environment }}&amp;#60;/p&amp;#62; &amp;#60;p&amp;#62;Base Url: {{ baseUrl }}&amp;#60;/p&amp;#62; &amp;#60;/div&amp;#62; &amp;#60;/template&amp;#62; &amp;#60;script&amp;#62; import Config from "./config.json"; export default { name: 'App', data: () =&amp;#62; { return { environment: Config.ENV, baseUrl: Config.BASE_URL } } } &amp;#60;/script&amp;#62; &lt;/pre&gt; &lt;h2&gt;Multi-stage build containers&lt;/h2&gt; &lt;p&gt;Now, you&amp;#8217;re ready to build the front-end container. For this process, you will use a container to create the production version of the application. Docker will then copy this build function&amp;#8217;s output into a second container, an NGINX server. Once the second container is created, you discard the first container. What&amp;#8217;s left is the NGINX server with the minimal set of files from the prior stage.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s start by creating an image to contain the application. Later, we&amp;#8217;ll come back to apply the environment variables. For this stage, you&amp;#8217;ll do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a new file called &lt;code&gt;Dockerfile&lt;/code&gt;. The first stage uses a &lt;code&gt;node:14&lt;/code&gt; image to build the production version of the application. Copy over all of your files into the container.&lt;/li&gt; &lt;li&gt;Copy the files, then run an &lt;code&gt;npm install&lt;/code&gt; to fetch the project&amp;#8217;s dependencies and run an &lt;code&gt;npm run build&lt;/code&gt; to create the production assets.&lt;/li&gt; &lt;li&gt;Start the second stage with a &lt;code&gt;FROM nginx:1.17&lt;/code&gt; statement and copy the files from the first stage into this new container.&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: To avoid copying unnecessary files such as the &lt;code&gt;node_modules&lt;/code&gt; folders, create a &lt;code&gt;.docker-ignore&lt;/code&gt; file in the same folder as your &lt;code&gt;Dockerfile&lt;/code&gt; and list the folders to ignore. Also, note that the production code&amp;#8217;s location varies based on the JavaScript framework you are using, so uncomment the line you need. Angular requires that you change the name of your project manually.&lt;/p&gt; &lt;p&gt;Here is the complete Dockerfile at this stage:&lt;/p&gt; &lt;pre&gt;FROM node:14 WORKDIR /app COPY . . RUN npm install &amp;#38;&amp;#38; npm run build FROM nginx:1.17 WORKDIR /usr/share/nginx/html # Angular # COPY --from=0 /app/dist/&amp;#60;projectName&amp;#62; . # React # COPY --from=0 /app/build . # VueJS # COPY --from=0 /app/dist . &lt;/pre&gt; &lt;p&gt;After creating the Dockerfile, you can build the image and start the container to test it out. Run the following commands and open your browser to &lt;a target="_blank" rel="nofollow" href="http://localhost:8080/"&gt;http://localhost:8080&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;docker build -t front-end. docker run -d -p 8080:80 --rm --name front frontend &lt;/pre&gt; &lt;p&gt;To stop the container after you&amp;#8217;ve tested it, enter:&lt;/p&gt; &lt;pre&gt;docker stop front&lt;/pre&gt; &lt;h2&gt;Inject the environment variables&lt;/h2&gt; &lt;p&gt;Next, you will edit the Dockerfile to inject your environment variables. First, you&amp;#8217;ll overwrite the content of your original &lt;code&gt;config.json&lt;/code&gt; file, then you&amp;#8217;ll tweak the NGINX server to inject the environment variables.&lt;/p&gt; &lt;h3&gt;Overwrite config.json&lt;/h3&gt; &lt;p&gt;Instead of having actual values, each property&amp;#8217;s value will be &amp;#8220;&lt;code&gt;$key&lt;/code&gt;&amp;#8220;. The resulting &lt;code&gt;config.json&lt;/code&gt; looks like this:&lt;/p&gt; &lt;pre&gt;{ ENV: "$ENV", BASE_URL: "$BASE_URL" } &lt;/pre&gt; &lt;p&gt;You will use the &lt;code&gt;envsubst&lt;/code&gt; to change the &lt;code&gt;$KEY&lt;/code&gt; values to the environment variable&amp;#8217;s real value just before the server starts. For this to work, you need to add instructions to the first step of the Dockerfile to include &lt;a target="_blank" rel="nofollow" href="https://stedolan.github.io/jq/manual/"&gt;jq&lt;/a&gt;, a tool that makes it easy to edit the contents of a JSON file from the CLI. Right after the &lt;code&gt;FROM&lt;/code&gt; line in your Dockerfile, add the following to install &lt;code&gt;jq&lt;/code&gt; in the container:&lt;/p&gt; &lt;pre&gt;ENV JQ_VERSION=1.6 RUN wget --no-check-certificate https://github.com/stedolan/jq/releases/download/jq-${JQ_VERSION}/jq-linux64 -O /tmp/jq-linux64 RUN cp /tmp/jq-linux64 /usr/bin/jq RUN chmod +x /usr/bin/jq &lt;/pre&gt; &lt;p&gt;After the files have been copied, you can use &lt;code&gt;jq&lt;/code&gt; to edit the &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;RUN jq 'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' ./src/config.json &amp;#62; ./src/config.tmp.json &amp;#38;&amp;#38; mv ./src/config.tmp.json ./src/config.json &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: If you want to learn more about the &lt;code&gt;jq&lt;/code&gt; filter used in this example and experiment with other options, you can run it in &lt;a target="_blank" rel="nofollow" href="https://jqterm.com"&gt;jqTerm&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Tweak the NGINX server&lt;/h3&gt; &lt;p&gt;After you&amp;#8217;ve modified the &lt;code&gt;config.json&lt;/code&gt; file, you will tweak the NGINX server to inject the environment variables. To do so, you will need to create a script to be executed before starting the NGINX server.&lt;/p&gt; &lt;p&gt;This file (&lt;code&gt;start-nginx.sh&lt;/code&gt;) contains quite a bit of bash scripting. The first line of the script runs a command to get the names of all existing environment variables and stores those in &lt;code&gt;$EXISTING_VARS&lt;/code&gt;. The script then loops through each JavaScript file in your production folder and replaces any &lt;code&gt;$VARIABLE&lt;/code&gt; with the actual value of that environment variable. Once it&amp;#8217;s done, it starts the NGINX server with the default command:&lt;/p&gt; &lt;pre&gt;#!/usr/bin/env bash export EXISTING_VARS=$(printenv | awk -F= '{print $1}' | sed 's/^/\$/g' | paste -sd,); for file in $JSFOLDER; do cat $file | envsubst $EXISTING_VARS | tee $file done nginx -g 'daemon off;' &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The location of the JavaScript files differs for each framework. The &lt;code&gt;$JSFOLDER&lt;/code&gt; variable is set in the Dockerfile so that you can uncomment the line you need there.&lt;/p&gt; &lt;p&gt;Now, add this file to the container and overwrite the NGINX image&amp;#8217;s default entry point with this new script. Right after the &lt;code&gt;FROM&lt;/code&gt; statement of the second stage, add the following lines for your framework:&lt;/p&gt; &lt;pre&gt;# Angular # ENV JSFOLDER=/usr/share/nginx/html/*.js # React # ENV JSFOLDER=/usr/share/nginx/html/static/js/*.js # VueJS # ENV JSFOLDER=/usr/share/nginx/html/js/*.js COPY ./start-nginx.sh /usr/bin/start-nginx.sh RUN chmod +x /usr/bin/start-nginx.sh &lt;/pre&gt; &lt;p&gt;At the very end of the file, add the new entry point:&lt;/p&gt; &lt;pre&gt;ENTRYPOINT [ "start-nginx.sh" ] &lt;/pre&gt; &lt;p&gt;Your final Dockerfile should look like this one. You can uncomment the required lines and remove all the other commented statements:&lt;/p&gt; &lt;pre&gt;FROM node:14 ENV JQ_VERSION=1.6 RUN wget --no-check-certificate https://github.com/stedolan/jq/releases/download/jq-${JQ_VERSION}/jq-linux64 -O /tmp/jq-linux64 RUN cp /tmp/jq-linux64 /usr/bin/jq RUN chmod +x /usr/bin/jq WORKDIR /app COPY . . RUN jq 'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' ./src/config.json &amp;#62; ./src/config.tmp.json &amp;#38;&amp;#38; mv ./src/config.tmp.json ./src/config.json RUN npm install &amp;#38;&amp;#38; npm run build FROM nginx:1.17 # Angular # ENV JSFOLDER=/usr/share/nginx/html/*.js # React # ENV JSFOLDER=/usr/share/nginx/html/static/js/*.js # VueJS # ENV JSFOLDER=/usr/share/nginx/html/js/*.js COPY ./start-nginx.sh /usr/bin/start-nginx.sh RUN chmod +x /usr/bin/start-nginx.sh WORKDIR /usr/share/nginx/html # Angular # COPY --from=0 /app/dist/&amp;#60;projectName&amp;#62; . # React # COPY --from=0 /app/build . # VueJS # COPY --from=0 /app/dist . ENTRYPOINT [ "start-nginx.sh" ] &lt;/pre&gt; &lt;h2&gt;Rebuild your image and start the server&lt;/h2&gt; &lt;p&gt;You are now ready to rebuild your image and start the server again, but this time with environment variables. Open your browser at &lt;a target="_blank" rel="nofollow" href="http://localhost:8080/"&gt;http://localhost:8080&lt;/a&gt;, and you should see the application running with the values of the environment variables you&amp;#8217;ve passed to Docker:&lt;/p&gt; &lt;pre&gt;docker build -t frontend . docker run -d -p 8080:80 --rm --name front -e ENV=prod -e BASE_URL=/api frontend &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In summary, here are the steps to make your environment variables accessible in your front-end containers:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Add a &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/frontend-containers/blob/main/config.json"&gt;config.json&lt;/a&gt; file in your &lt;code&gt;/src&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;Add the &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/frontend-containers/blob/main/start-nginx.sh"&gt;start-nginx.sh&lt;/a&gt; bash script to your project.&lt;/li&gt; &lt;li&gt;Use the completed &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/frontend-containers/blob/main/Dockerfile"&gt;Dockerfile&lt;/a&gt; to build your project.&lt;/li&gt; &lt;li&gt;Start your container using &lt;code&gt;-e&lt;/code&gt; to specify the environment variables.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Once you&amp;#8217;ve created a Dockerfile following these steps, you can reuse it for any of your JavaScript projects. All the variables in the &lt;code&gt;config.json&lt;/code&gt; will change automatically, and you won&amp;#8217;t need to think about them anymore. You can find the complete source code and examples for the Angular, React, and Vue.js applications used in this article on &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/frontend-containers"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#38;linkname=Making%20environment%20variables%20accessible%20in%20front-end%20containers" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fmaking-environment-variables-accessible-in-front-end-containers%2F&amp;#038;title=Making%20environment%20variables%20accessible%20in%20front-end%20containers" data-a2a-url="https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/" data-a2a-title="Making environment variables accessible in front-end containers"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/"&gt;Making environment variables accessible in front-end containers&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bRQAOiM1CwA" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;When building a container for a single-page application using any modern JavaScript framework (such as Angular, React, or Vue.js), you might find that the configuration settings are different depending on where the container will run. A typical case would be the base URL for your API, which will differ depending on whether you are testing [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/"&gt;Making environment variables accessible in front-end containers&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">861157</post-id><dc:creator>Joel Lord</dc:creator><dc:date>2021-03-04T18:59:36Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/</feedburner:origLink></entry><entry><title>Building rootless containers for JavaScript front ends</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/u_E8Yo1oe3o/" /><category term="Containers" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="front end javascript" /><category term="nginx" /><category term="openshift" /><category term="rootless" /><category term="rootless container" /><author><name>Joel Lord</name></author><id>https://developers.redhat.com/blog/?p=861477</id><updated>2021-03-04T08:00:09Z</updated><published>2021-03-04T08:00:09Z</published><content type="html">&lt;p&gt;By default, most &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; are run as the root user. It is much easier to install dependencies, edit files, and run processes on restricted ports when they run as root. As is usually the case in computer science, though, simplicity comes at a cost. In this case, containers run as root are more vulnerable to malicious code and attacks. To avoid those potential &lt;a href="https://developers.redhat.com/topics/security"&gt;security&lt;/a&gt; gaps, &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; won&amp;#8217;t let you run containers as a root user. This restriction adds a layer of security and isolates the containers.&lt;/p&gt; &lt;p&gt;This article shows you how to run a JavaScript front-end application in a rootless container. The example builds on the code from my previous article, &lt;a href="https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers"&gt;&lt;i&gt;Making environment variables accessible in front-end containers&lt;/i&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Building a rootless container&lt;/h2&gt; &lt;p&gt;Here is the Dockerfile we&amp;#8217;ll use for our example. As demonstrated in my &lt;a href="https://developers.redhat.com/blog/2021/03/04/making-environment-variables-accessible-in-front-end-containers/"&gt;previous article&lt;/a&gt;, you can use this Dockerfile to access environment variables from your Angular, React, or Vue.js applications:&lt;/p&gt; &lt;pre&gt;FROM node:14 ENV JQ_VERSION=1.6 RUN wget --no-check-certificate https://github.com/stedolan/jq/releases/download/jq-${JQ_VERSION}/jq-linux64 -O /tmp/jq-linux64 RUN cp /tmp/jq-linux64 /usr/bin/jq RUN chmod +x /usr/bin/jq WORKDIR /app COPY . . RUN jq 'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' ./src/config.json | ./src/config.tmp.json &amp;#38;&amp;#38; mv ./src/config.tmp.json config.json RUN npm install &amp;#38;&amp;#38; npm run build FROM nginx:1.17 # Angular: ENV JSFOLDER=/usr/share/nginx/html/*.js # React: ENV JSFOLDER=/usr/share/nginx/html/static/js/*.js # VueJS: ENV JSFOLDER=/usr/share/nginx/html/js/*.js COPY ./start-nginx.sh /usr/bin/start-nginx.sh RUN chmod +x /usr/bin/start-nginx.sh WORKDIR /usr/share/nginx/html # Angular: COPY --from=0 /app/dist/ . # React: COPY --from=0 /app/build . # VueJS: COPY --from=0 /app/dist . ENTRYPOINT [ "start-nginx.sh" ] &lt;/pre&gt; &lt;p&gt;This container uses two stages to build the final container. In the first stage, it uses the &lt;code&gt;node:14&lt;/code&gt; image, which is running as root. The build process will eventually discard this container, so you don&amp;#8217;t need to worry about it.&lt;/p&gt; &lt;p&gt;The second-stage container is the one that needs to be secured. The &lt;code&gt;nginx&lt;/code&gt; base image is currently running as root, mainly so that it can run on port 80, which requires privileged access to enable. Once this container is ready to run rootless, it will run on port 8080. You will need to change the default &lt;code&gt;nginx&lt;/code&gt; configuration for the container to run rootless. You will also need to make sure that the server itself is running as an unprivileged user. Finally, the user will need access to several files and folders.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s get started with making this container a rootless one.&lt;/p&gt; &lt;h2&gt;Create the NGINX configuration file&lt;/h2&gt; &lt;p&gt;The first step is to create a new configuration file for NGINX. You can start with the most basic configuration file needed to run NGINX and build it from there:&lt;/p&gt; &lt;pre&gt;worker_processes auto; events { worker_connections 1024; } http { include /etc/nginx/mime.types; server { server_name _; index index.html; location / { try_files $uri /index.html; } } }&lt;/pre&gt; &lt;p&gt;Next, you need to change the server settings to run on port 8080 instead of the default port 80. You&amp;#8217;ll also need to change the default path that NGINX uses to serve files:&lt;/p&gt; &lt;pre&gt;http { ... server { listen 8080; ... location / { root /code; ... } } }&lt;/pre&gt; &lt;p&gt;The final &lt;code&gt;nginx.conf&lt;/code&gt; file should look like this:&lt;/p&gt; &lt;pre&gt;worker_processes auto; events { worker_connections 1024; } http { include /etc/nginx/mime.types; server { listen 8080; server_name _; index index.html; location / { root /opt/app; try_files $uri /index.html; } } }&lt;/pre&gt; &lt;h2&gt;Edit the Dockerfile&lt;/h2&gt; &lt;p&gt;Now that you have a new NGINX configuration file that lets the server run as a regular user, it&amp;#8217;s time to edit the Dockerfile. This modified container will run as user &lt;code&gt;nginx&lt;/code&gt;. In this case, the NGINX base images provide the non-root user.&lt;/p&gt; &lt;p&gt;In the second step of your build, right after you&amp;#8217;ve specified your base image with the &lt;code&gt;FROM&lt;/code&gt; statement, you can copy your new NGINX configuration file to overwrite the default one. Then, create an &lt;code&gt;/opt/app&lt;/code&gt; folder and change its ownership:&lt;/p&gt; &lt;pre&gt;FROM nginx:1.17 COPY ./nginx.conf /etc/nginx/nginx.conf RUN mkdir -p /opt/app &amp;#38;&amp;#38; chown -R nginx:nginx /opt/app &amp;#38;&amp;#38; chmod -R 775 /opt/app &lt;/pre&gt; &lt;p&gt;Don&amp;#8217;t forget to change the &lt;code&gt;JSFOLDER&lt;/code&gt; variable. This will ensure that your environment variables are still injected by the bash script.&lt;/p&gt; &lt;pre&gt;# Angular # ENV JSFOLDER=/opt/app/*.js # React # ENV JSFOLDER=/opt/app/static/js/*.js # VueJS # ENV JSFOLDER=/opt/app/js/*.js &lt;/pre&gt; &lt;h3&gt;Change the file ownership&lt;/h3&gt; &lt;p&gt;Next, you need to give NGINX access to run a series of files and folders for caching and logging purposes. You can change the ownership of all of them in a single &lt;code&gt;RUN&lt;/code&gt; statement, using ampersands to chain the commands:&lt;/p&gt; &lt;pre&gt;RUN chown -R nginx:nginx /var/cache/nginx &amp;#38;&amp;#38; \ chown -R nginx:nginx /var/log/nginx &amp;#38;&amp;#38; \ chown -R nginx:nginx /etc/nginx/conf.d &lt;/pre&gt; &lt;p&gt;NGINX also requires an &lt;code&gt;nginx.pid&lt;/code&gt; file. This file does not exist yet, so you need to create it and assign ownership to the &lt;code&gt;nginx&lt;/code&gt; user:&lt;/p&gt; &lt;pre&gt;RUN touch /var/run/nginx.pid &amp;#38;&amp;#38; \ chown -R nginx:nginx /var/run/nginx.pid &lt;/pre&gt; &lt;h3&gt;Update the group and permissions&lt;/h3&gt; &lt;p&gt;Finally, you will change the group for those files and folders and change the permissions so that NGINX can read and write the folders:&lt;/p&gt; &lt;pre&gt;RUN chgrp -R root /var/cache/nginx /var/run /var/log/nginx /var/run/nginx.pid &amp;#38;&amp;#38; \ chmod -R 775 /var/cache/nginx /var/run /var/log/nginx /var/run/nginx.pid &lt;/pre&gt; &lt;h3&gt;Switch to the rootless user&lt;/h3&gt; &lt;p&gt;Now that you&amp;#8217;ve adjusted all the permissions, you can tell Docker to switch over to the &lt;code&gt;nginx&lt;/code&gt; user using the &lt;code&gt;USER&lt;/code&gt; statement. You can then copy the files from the builder step into the &lt;code&gt;/opt/app&lt;/code&gt; folder using the &lt;code&gt;--chown&lt;/code&gt; flag, which makes the files accessible by the &lt;code&gt;nginx&lt;/code&gt; user. Finally, you will tell Docker that this new image uses a different port. Use the &lt;code&gt;EXPOSE&lt;/code&gt; statement for port 8080:&lt;/p&gt; &lt;pre&gt;USER nginx WORKDIR /opt/app COPY --from=builder --chown=nginx . RUN chmod -R a+rw /opt/app EXPOSE 8080 &lt;/pre&gt; &lt;p&gt;The final front-end Dockerfile will look like this:&lt;/p&gt; &lt;pre&gt;FROM node:14 ENV JQ_VERSION=1.6 RUN wget --no-check-certificate https://github.com/stedolan/jq/releases/download/jq-${JQ_VERSION}/jq-linux64 -O /tmp/jq-linux64 RUN cp /tmp/jq-linux64 /usr/bin/jq RUN chmod +x /usr/bin/jq WORKDIR /app COPY . . RUN jq 'to_entries | map_values({ (.key) : ("$" + .key) }) | reduce .[] as $item ({}; . + $item)' ./src/config.json | ./src/config.tmp.json &amp;#38;&amp;#38; mv ./src/config.tmp.json config.json RUN npm install &amp;#38;&amp;#38; npm run build FROM nginx:1.17 # Angular # ENV JSFOLDER=/opt/app/*.js # React # ENV JSFOLDER=/opt/app/static/js/*.js # VueJS # ENV JSFOLDER=/opt/app/js/*.js COPY ./nginx.conf /etc/nginx/nginx.conf RUN mkdir -p /opt/app &amp;#38;&amp;#38; chown -R nginx:nginx /opt/app &amp;#38;&amp;#38; chmod -R 775 /opt/app RUN chown -R nginx:nginx /var/cache/nginx &amp;#38;&amp;#38; \ chown -R nginx:nginx /var/log/nginx &amp;#38;&amp;#38; \ chown -R nginx:nginx /etc/nginx/conf.d RUN touch /var/run/nginx.pid &amp;#38;&amp;#38; \ chown -R nginx:nginx /var/run/nginx.pid RUN chgrp -R root /var/cache/nginx /var/run /var/log/nginx /var/run/nginx.pid &amp;#38;&amp;#38; \ chmod -R 775 /var/cache/nginx /var/run /var/log/nginx /var/run/nginx.pid COPY ./start-nginx.sh /usr/bin/start-nginx.sh RUN chmod +x /usr/bin/start-nginx.sh EXPOSE 8080 WORKDIR /opt/app # Angular # COPY --from=0 --chown=nginx /app/dist/ . # React # COPY --from=0 /app/build . # VueJS # COPY --from=0 /app/dist . RUN chmod -R a+rw /opt/app USER nginx ENTRYPOINT [ "start-nginx.sh" ]&lt;/pre&gt; &lt;p&gt;Your new Dockerfile is ready to go! You can test it out by using a &lt;code&gt;docker build&lt;/code&gt; followed by a &lt;code&gt;docker run&lt;/code&gt;. Don&amp;#8217;t forget to map the new port since this container doesn&amp;#8217;t run on port 80 anymore:&lt;/p&gt; &lt;pre&gt;docker build -t frontend . docker run -d -p 8080:8080 --rm --name front -e ENV=prod -e BASE_URL=/api frontend &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;You now have everything needed to run your JavaScript front end in a secure container. You can reuse the image we developed in this article for all of your JavaScript projects, whether you are using Angular, React, or Vue.js. The front end not only runs securely but also lets you inject environment variables into your code. You can find all the examples and source code from this article on &lt;a target="_blank" rel="nofollow" href="http://github.com/joellord/frontend-containers"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#38;linkname=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F04%2Fbuilding-rootless-containers-for-javascript-front-ends%2F&amp;#038;title=Building%20rootless%20containers%20for%20JavaScript%20front%20ends" data-a2a-url="https://developers.redhat.com/blog/2021/03/04/building-rootless-containers-for-javascript-front-ends/" data-a2a-title="Building rootless containers for JavaScript front ends"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/04/building-rootless-containers-for-javascript-front-ends/"&gt;Building rootless containers for JavaScript front ends&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/u_E8Yo1oe3o" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;By default, most containers are run as the root user. It is much easier to install dependencies, edit files, and run processes on restricted ports when they run as root. As is usually the case in computer science, though, simplicity comes at a cost. In this case, containers run as root are more vulnerable to [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/04/building-rootless-containers-for-javascript-front-ends/"&gt;Building rootless containers for JavaScript front ends&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/04/building-rootless-containers-for-javascript-front-ends/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">861477</post-id><dc:creator>Joel Lord</dc:creator><dc:date>2021-03-04T08:00:09Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/04/building-rootless-containers-for-javascript-front-ends/</feedburner:origLink></entry><entry><title type="html">Business optimisation architecture - Example vaccine scheduling</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3NJxyvEvNik/business-optimisation-architecture-example-vaccine-scheduling-.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/p3YFp6gjxCA/business-optimisation-architecture-example-vaccine-scheduling-.html</id><updated>2021-03-04T06:00:00Z</updated><content type="html">Part 4 - Example vaccine scheduling In  we looked at an example architecture for retail planning optimisation. It started with laying out the process of how I've approached the use case by researching successful customer portfolio solutions as the basis for a generic architectural blueprint. The specific example of how retail organisations can optimise delivery planning, employee rostering, and optimise task assignments was laid out in the architecture blueprint diagram. This article continues on with another specific example that focuses on how retail stores around the world are helping deliver vaccinations through their pharmacies. It walks you through an example optimisation scenario showing how to provide for customer vaccine scheduling. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's my intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but I've chosen a format that I hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution. VACCINE SCHEDULING ARCHITECTURE The example blueprint shown in the figure titled Schematic - Business optimisation (vaccine scheduling) outlines how this type of optimisation ties into your architecture. In this example, starting from the left we see a business owner and developer providing the input needed for the vaccine planning services. These inputs are constraints (both hard and soft), resource availability, and business goals to be achieved.  It's interesting to take a short side trip into the reason that various constraints make putting together a schedule so demanding that special tooling is needed. In the case of vaccination scheduling there are things you need to think about such as: * planning runs need to complete in timely fashion, for example: * ~350k slots should produce a schedule within an hour * runs can be done several times a day * if anyone with a slot cancels, the planning needs to stop it's run and start again * achieve continuous planning * need rules to prevent games the system, for example: * can't request a slot, then cancel if not satisfied with your slot, thereby getting an earlier slot * ensure cancelled slots in schedule go to back of the line * account for vaccine types that can't be given to certain age groups * give priority to second vaccine slot planning over first vaccine slot These are just some of the constraints and conditions that need to be met with regards to designing and executing a planning cycle. Something to think about, right? While this might look like something that the business owner and developer are doing into the fully deployed solution, it's really using the previously covered  showcased in that blueprint. For simplicity, we've included the planning and constraint development aspects here to help with an understanding that business owners are involved. The vaccine planning services can then be triggered or viewed by external systems shown as planners that have been given access through the API management element to start, provide input, or retreive planning optimisation results.  The integration microservices are making extensive use of the planning results to share a vaccine appointment with the user of the frontend application, shown here as a mobile application. Data access is shown at the bottom making use of vaccine center data, vaccine supply data, and customer data. Access for the vaccine planning services is arranged by the integration data microservices allowing for clean separation of integration points between critical demarcation lines of your architecture. While the business owner and developer are working on the constraints and modeling of the needed vaccine planning services, at runtime the rest of the elements in this diagram are leveraging these optimisation planning services to achieve desired outcomes.  The diagram might give the impression that this is a single in store pharmacy solution, but it can also be seen in the context of a centralised architecture in the retail organisation where the external status views are those of satellite stores or warehouses. The stores and warehouses are all looking to make use of the vaccine planning and optimising services for better pharmacy vaccine scheduling. WHAT'S NEXT THIS WAS JUST A SHORT OVERVIEW OF A VACCINE SCHEDULING ARCHITECTURE THAT PROVIDES YOU WITH A MAP TO SOLVE YOUR OWN BUSINESS OPTIMISATION CHALLENGES.  AN OVERVIEW OF THIS SERIES ON THE BUSINESS OPTIMISATION PORTFOLIO ARCHITECTURE BLUEPRINT CAN BE FOUND HERE: 1. 2. 3. 4. CATCH UP ON ANY ARTICLES YOU MISSED BY FOLLOWING ONE OF THE LINKS ABOVE. THIS COMPLETES THE SERIES AND WE HOPE YOU ENJOYED THIS ARCHITECTURE BLUEPRINT FOR BUSINESS OPTIMISATION. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3NJxyvEvNik" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/p3YFp6gjxCA/business-optimisation-architecture-example-vaccine-scheduling-.html</feedburner:origLink></entry></feed>
