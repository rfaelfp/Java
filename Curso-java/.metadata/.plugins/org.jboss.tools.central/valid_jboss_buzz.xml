<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Command-line cluster management with Red Hat OpenShift’s new web terminal (tech preview)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/swGMQMl_psE/" /><category term="devops" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="linux" scheme="searchisko:content:tags" /><category term="odo" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="OpenShift cluster" scheme="searchisko:content:tags" /><category term="OpenShift Operator" scheme="searchisko:content:tags" /><category term="operator" scheme="searchisko:content:tags" /><category term="Web Terminal Operator" scheme="searchisko:content:tags" /><author><name>Joshua Wood</name></author><id>searchisko:content:id:jbossorg_blog-command_line_cluster_management_with_red_hat_openshift_s_new_web_terminal_tech_preview</id><updated>2020-10-01T07:00:29Z</updated><published>2020-10-01T07:00:29Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&amp;#8216;s web console simplifies many development and deployment chores to just a few clicks, but sometimes you need a command-line interface (CLI) to get things done on a cluster. Whether you&amp;#8217;re learning by cut-and-paste in a tutorial or troubleshooting a deep bug in production (also often done by cut-and-paste), you’ll likely need to enter at least a line or two at a command prompt.&lt;/p&gt; &lt;p&gt;Starting with version 4.5.3, OpenShift users can try out a tech preview of the new Web Terminal Operator. The new OpenShift web terminal brings indispensable command-line tools right to the web console, and its Linux environment runs in a pod deployed on your OpenShift cluster. The web terminal eliminates the need to install software and configure connections and authentication for your local terminal. It also makes it easier to use OpenShift on devices like tablets and mobile phones, which might lack a native terminal.&lt;/p&gt; &lt;p&gt;&lt;span id="more-787327"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;This article introduces the new OpenShift web terminal, including how to install and activate the Web Terminal Operator.&lt;/p&gt; &lt;h2&gt;An easier way to manage OpenShift clusters&lt;/h2&gt; &lt;p&gt;The new OpenShift web terminal includes key programs for working with clusters, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; tool for comprehensive OpenShift management.&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo&lt;/code&gt;, OpenShift&amp;#8217;s streamlined workflow utility for application development.&lt;/li&gt; &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt;, the core Kubernetes API client.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;CLI client tools for the &lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-cloud-native-ci-cd-with-tekton-kubecon-europe-2020/"&gt;Tekton CI/CD framework&lt;/a&gt;, &lt;a href="https://developers.redhat.com/blog/2020/07/20/advanced-helm-support-in-the-openshift-4-5-web-console/"&gt;Helm application deployment charts&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Knative serverless workloads&lt;/a&gt; are also installed and ready to run. These OpenShift and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; tools are supported by the usual suspects of Unix-like, general-purpose text processing, and shell scripting.&lt;/p&gt; &lt;p&gt;Once you&amp;#8217;ve installed the Web Terminal Operator, you can access the web terminal from the command-prompt icon (&lt;strong&gt;&amp;#62;_&lt;/strong&gt;) on the OpenShift web console masthead, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_787737" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-787737" class="wp-image-787737 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/wticon.png" alt="A screenshot showing the command-prompt icon in the web console." width="640" height="148" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/wticon.png 640w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/wticon-300x69.png 300w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-787737" class="wp-caption-text"&gt;Figure 1: The command prompt icon in the OpenShift web console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Clicking the icon displays the web terminal frame at the bottom of the OpenShift web console, as shown in Figure 2. You can resize, reposition, or pop out the terminal into a new browser window or tab.&lt;/p&gt; &lt;div id="attachment_787747" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-787747" class="wp-image-787747 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/wtrunning-1024x502.png" alt="A screenshot of the web terminal open in the web console." width="640" height="314" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/wtrunning-1024x502.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/wtrunning-300x147.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/wtrunning-768x377.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-787747" class="wp-caption-text"&gt;Figure 2: The new command-line terminal opens at the bottom of the OpenShift web console.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;How to activate the OpenShift web terminal&lt;/h2&gt; &lt;p&gt;OpenShift versions 4.5.3 and later support the new Web Terminal Operator, which manages the terminal environment on the cluster. To activate the web terminal, visit the &lt;b&gt;OperatorHub&lt;/b&gt; in the left sidebar of the &lt;b&gt;Web Console Administrator Experience&lt;/b&gt; and search for &lt;b&gt;Web Terminal&lt;/b&gt;. Install the Web Terminal Operator.&lt;/p&gt; &lt;p&gt;Once you&amp;#8217;ve deployed the Operator, log into the web console as a user without the cluster-admin role. Click the command-prompt icon in the top-right corner of your screen to start a web terminal. You’re ready to build, deploy, and manage your cluster workloads with your favorite &lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;odo&lt;/code&gt; one-liners, without ever leaving your browser.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The web terminal&amp;#8217;s tech preview imposes a couple of limitations. First, cluster admins cannot use the web terminal, it is available only to less privileged roles. Second, the shell history feature works to recall previous commands with the up and down arrow keys and other bash mechanics, but this information is not preserved between terminal sessions.&lt;/p&gt; &lt;h2&gt;We appreciate your feedback&lt;/h2&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience. We really want to hear from you! Attend one of our office hours, or &lt;a target="_blank" rel="nofollow" href="https://forms.gle/zDd4tuWvjndCRVMD8"&gt;complete this survey&lt;/a&gt; to let us know your thoughts about the OpenShift web console and the new web terminal. You can also join the &lt;a target="_blank" rel="nofollow" href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google Group&lt;/a&gt; to share your tips, get help with what doesn’t work so well for you, and shape the future of the OpenShift Developer Experience.&lt;/p&gt; &lt;p&gt;Ready to get started? &lt;a target="_blank" rel="nofollow" href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#38;linkname=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fcommand-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview%2F&amp;#038;title=Command-line%20cluster%20management%20with%20Red%20Hat%20OpenShift%E2%80%99s%20new%20web%20terminal%20%28tech%20preview%29" data-a2a-url="https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/" data-a2a-title="Command-line cluster management with Red Hat OpenShift’s new web terminal (tech preview)"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/"&gt;Command-line cluster management with Red Hat OpenShift&amp;#8217;s new web terminal (tech preview)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/swGMQMl_psE" height="1" width="1" alt=""/&gt;</content><summary>Red Hat OpenShift‘s web console simplifies many development and deployment chores to just a few clicks, but sometimes you need a command-line interface (CLI) to get things done on a cluster. Whether you’re learning by cut-and-paste in a tutorial or troubleshooting a deep bug in production (also often done by cut-and-paste), you’ll likely need to enter at least a line or two at a command prompt. St...</summary><dc:creator>Joshua Wood</dc:creator><dc:date>2020-10-01T07:00:29Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/</feedburner:origLink></entry><entry><title>Building modern CI/CD workflows for serverless applications with Red Hat OpenShift Pipelines and Argo CD, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/eafqH_hG3wo/" /><category term="ArgoCD" scheme="searchisko:content:tags" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="ci/cd pipeline" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="gitops" scheme="searchisko:content:tags" /><category term="Knative" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><category term="Tekton" scheme="searchisko:content:tags" /><author><name>David Sancho</name></author><id>searchisko:content:id:jbossorg_blog-building_modern_ci_cd_workflows_for_serverless_applications_with_red_hat_openshift_pipelines_and_argo_cd_part_1</id><updated>2020-10-01T07:00:27Z</updated><published>2020-10-01T07:00:27Z</published><content type="html">&lt;p&gt;A recent article, &lt;a href="https://developers.redhat.com/blog/2020/09/03/the-present-and-future-of-ci-cd-with-gitops-on-red-hat-openshift/"&gt;&lt;em&gt;The present and future of CI/CD with GitOps on Red Hat OpenShift&lt;/em&gt;&lt;/a&gt;, proposed &lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-cloud-native-ci-cd-with-tekton-kubecon-europe-2020/"&gt;Tekton&lt;/a&gt; as a framework for cloud-native CI/CD pipelines, and Argo CD as its perfect partner for GitOps. &lt;a href="https://developers.redhat.com/devnation/tech-talks/gitops/watch"&gt;GitOps&lt;/a&gt; practices support continuous delivery in hybrid, multi-cluster &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;In this two-part article, we&amp;#8217;ll build a CI/CD workflow that demonstrates the potential of combining Tekton and GitOps. You&amp;#8217;ll also be introduced to &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, as we&amp;#8217;ll use &lt;a target="_blank" rel="nofollow" href="https://knative.dev/"&gt;Knative&lt;/a&gt; service resources in our CI/CD workflow. Let&amp;#8217;s start with an overview of the CI/CD workflow that we&amp;#8217;ll implement for the demonstration.&lt;/p&gt; &lt;h2&gt;The CI/CD workflow&lt;/h2&gt; &lt;p&gt;The diagram in Figure 1 illustrates the CI/CD workflow. A commit initiated in the application&amp;#8217;s source code repository triggers a full CI/CD process, which ends with a new version of the serverless application deployed in development, staging, and production environments as laid out in Figure 1.&lt;/p&gt; &lt;div id="attachment_780177" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/cicd-knative.png"&gt;&lt;img aria-describedby="caption-attachment-780177" class="wp-image-780177" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/cicd-knative.png" alt="A diagram of the sample CI/CI workflow." width="640" height="520" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/cicd-knative.png 829w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/cicd-knative-300x244.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/cicd-knative-768x623.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780177" class="wp-caption-text"&gt;Figure 1: The sample CI/CD workflow for the demonstration.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Let&amp;#8217;s look more closely at each step in the workflow:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A developer pushes a new change in the application&amp;#8217;s source code repository.&lt;/li&gt; &lt;li&gt;A webhook configured in the source code repository (GitHub, in this case) triggers the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline"&gt;Tekton pipeline&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Once the pipeline has started, the first task fetches the source code from the repository.&lt;/li&gt; &lt;li&gt;A Maven task packages the application code as a JAR file and runs unit tests before building the container image.&lt;/li&gt; &lt;li&gt;A &lt;a target="_blank" rel="nofollow" href="https://buildah.io/"&gt;buildah&lt;/a&gt; task builds and pushes the container image to the registry. The image is then pushed to the OpenShift internal registry.&lt;/li&gt; &lt;li&gt;The pipeline fetches the repository that keeps the desired state of the example application&amp;#8217;s configuration and deployment descriptors. In GitOps methodology, we use a Git repository as the single source of truth for what is deployed and where it&amp;#8217;s deployed.&lt;/li&gt; &lt;li&gt;Initially, the Git repository might be empty, so this task is smart enough to initialize the repository with all of the Kubernetes manifests (in this case, the &lt;a target="_blank" rel="nofollow" href="https://knative.dev/"&gt;Knative&lt;/a&gt; service and &lt;code&gt;ConfigMaps&lt;/code&gt;) that are required to run the application for the first time. The subsequent repository commits will only update the existing descriptors with the new application version, an independent route for canary testing, and related configurations. Once all the manifest files have been created or modified, this task pushes the changes to the repository. This step is the glue between the continuous integration performed by the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline"&gt;Tekton pipeline&lt;/a&gt; and the continuous deployment managed by &lt;a target="_blank" rel="nofollow" href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Argo CD pulls from the configuration repository and synchronizes the existing Kubernetes manifests, which are specified using &lt;a target="_blank" rel="nofollow" href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; files. This action creates the final Kubernetes objects in the &lt;code&gt;development&lt;/code&gt;, &lt;code&gt;staging&lt;/code&gt;, and &lt;code&gt;production&lt;/code&gt; namespaces. The synchronization could be auto or manual based on the target namespace&amp;#8217;s requirements.&lt;/li&gt; &lt;li&gt;In this final part of the workflow, it might be necessary to pull images referenced in the deployment Kubernetes manifest from the OpenShift internal registry. The operations team might also push configuration changes, for instance, changing the URL of a target microservice or certain information that is unknown by the development team. This last step could also create an &lt;code&gt;OutOfSync&lt;/code&gt; state in &lt;a target="_blank" rel="nofollow" href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt;, which would lead to a new synchronization process (see Step 9 in Figure 1).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Next, we&amp;#8217;ll set up our cluster with the OpenShift Operators and services that we&amp;#8217;ll need.&lt;/p&gt; &lt;h2&gt;Configuring the OpenShift cluster&lt;/h2&gt; &lt;p&gt;We&amp;#8217;ll use a set of scripts to configure and install all of the components required for this demonstration. To get started with setting up the demonstration environment, clone the following source code repository:&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/dsanchor/rh-developers-cicd.git &lt;/pre&gt; &lt;p&gt;Next, ensure that you have all of the following tools installed in your system. You&amp;#8217;ll need these pre-installed when you run the scripts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://helm.sh/docs/intro/install/"&gt;Helm&lt;/a&gt;: &lt;code&gt;helm &lt;em&gt;version&lt;/em&gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git"&gt;Git&lt;/a&gt;: &lt;code&gt;git &lt;em&gt;version&lt;/em&gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/cli_reference/openshift_cli/getting-started-cli.html"&gt;oc&lt;/a&gt;: &lt;code&gt;oc &lt;em&gt;version&lt;/em&gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes-sigs.github.io/kustomize/installation/"&gt;kustomize&lt;/a&gt; v 3.1.0 or higher: &lt;code&gt;customize &lt;em&gt;version&lt;/em&gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;envsubst (gettext): &lt;code&gt;envsubst --help&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/cli"&gt;tkn&lt;/a&gt; (optional Tekton CLI): &lt;code&gt;tkn &lt;em&gt;version&lt;/em&gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you&amp;#8217;ve checked the above requirements, log in to your OpenShift cluster as a cluster-admin user:&lt;/p&gt; &lt;pre&gt;$ oc login -u &lt;em&gt;USERNAME&lt;/em&gt; -p &lt;em&gt;PASSWORD&lt;/em&gt; https://api.&lt;em&gt;YOUR_CLUSTER_DOMAIN&lt;/em&gt;:6443 &lt;/pre&gt; &lt;h3&gt;Operators, namespaces, and role bindings&lt;/h3&gt; &lt;p&gt;Initially, we will install the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/pipelines/installing-pipelines.html"&gt;OpenShift Pipelines&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/serverless/installing_serverless/installing-openshift-serverless.html"&gt;OpenShift Serverless&lt;/a&gt; Operators in the &lt;code&gt;openshift-operators&lt;/code&gt; namespace.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll also create four new namespaces: &lt;code&gt;cicd&lt;/code&gt;, &lt;code&gt;development&lt;/code&gt;, &lt;code&gt;staging&lt;/code&gt;, and &lt;code&gt;production&lt;/code&gt;. Images are pushed within the boundaries of the &lt;code&gt;cicd&lt;/code&gt; namespace, so all of the other namespaces require &lt;code&gt;system:image-puller&lt;/code&gt; privileges in order to pull the new images.&lt;/p&gt; &lt;p&gt;Finally, we&amp;#8217;ll add a new &lt;code&gt;view&lt;/code&gt; role to the &lt;code&gt;development&lt;/code&gt;, &lt;code&gt;staging&lt;/code&gt;, and &lt;code&gt;production&lt;/code&gt; default service accounts. This role provides access from our &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; application pods to &lt;code&gt;ConfigMaps&lt;/code&gt; and &lt;code&gt;Secrets&lt;/code&gt;. (I&amp;#8217;ll introduce the Quarkus application later.)&lt;/p&gt; &lt;p&gt;Here is the script, which basically uses three Helm charts for the required installations:&lt;/p&gt; &lt;pre&gt;$ ./bootstrap.sh --------------- Installing openshift-pipelines operator Release "openshift-pipelines" does not exist. Installing it now. NAME: openshift-pipelines LAST DEPLOYED: Thu Sep 10 10:55:14 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Installing openshift-serverless Release "openshift-serverless" does not exist. Installing it now. NAME: openshift-serverless LAST DEPLOYED: Thu Sep 10 10:55:16 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Creating cicd, development, staging and production namespaces Added cicd system:image-puller role to default sa in development, staging and production namespaces Added view role to default sa in development, staging and production namespaces Release "bootstrap-projects" does not exist. Installing it now. NAME: bootstrap-projects LAST DEPLOYED: Thu Sep 10 10:55:18 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None &lt;/pre&gt; &lt;p&gt;You can execute the script as-is, or use the Helm charts independently, overriding any values that you wish. For instance, you could override the value of the channel subscription for each OpenShift Operator.&lt;/p&gt; &lt;p&gt;Figure 2 shows the installation so far, with both Operators installed under the &lt;code&gt;openshift-operators&lt;/code&gt; namespace.&lt;/p&gt; &lt;div id="attachment_780277" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/installed-openshift-operators.png"&gt;&lt;img aria-describedby="caption-attachment-780277" class="wp-image-780277 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/installed-openshift-operators-1024x253.png" alt="A screenshot of the OpenShift Serverless and OpenShift Pipelines Operators listed in the openshift-operators namespace." width="640" height="158" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/installed-openshift-operators-1024x253.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/installed-openshift-operators-300x74.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/installed-openshift-operators-768x190.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780277" class="wp-caption-text"&gt;Figure 2: The OpenShift Serverless and OpenShift Pipelines Operators installed under openshift-operators.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Verify that the OpenShift Pipelines Operator is installed at version 1.1.1 or greater.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll complete the OpenShift Serverless components installation by installing the &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/"&gt;Knative Serving&lt;/a&gt; control plane.&lt;/p&gt; &lt;h3&gt;Install a Knative Serving instance&lt;/h3&gt; &lt;p&gt;We need to create a &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/serverless/installing_serverless/installing-knative-serving.html#installing-knative-serving"&gt;Knative Serving&lt;/a&gt; instance that will provide a set of serverless capabilities to our applications. Run the following to create the Knative Serving instance and install the control plane:&lt;/p&gt; &lt;pre&gt;$ ./add-knative-serving.sh ------------------------------ Creating knative-serving namespace namespace/knative-serving created Installing basic knative serving control plane knativeserving.operator.knative.dev/knative-serving created &lt;/pre&gt; &lt;p&gt;We&amp;#8217;ve deployed a set of pods representing a basic Knative Serving control plane in the &lt;code&gt;knative-serving&lt;/code&gt; namespace, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_778117" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ns-1.png"&gt;&lt;img aria-describedby="caption-attachment-778117" class="wp-image-778117 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ns-1-1024x498.png" alt="A screenshot of the Knative serving control plane." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ns-1-1024x498.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ns-1-300x146.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ns-1-768x374.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-778117" class="wp-caption-text"&gt;Figure 3: The Knative Serving control plane in the knative-serving namespace.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;As shown in Figure 4, we&amp;#8217;ve also created a new namespace, &lt;code&gt;knative-serving-ingress&lt;/code&gt;, for the Knative installation&amp;#8217;s ingress gateways.&lt;/p&gt; &lt;div id="attachment_778087" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ingress.png"&gt;&lt;img aria-describedby="caption-attachment-778087" class="wp-image-778087 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ingress-1024x332.png" alt="A screenshot of the knative-serving-ingress namespace in the OpenShift console." width="640" height="208" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ingress-1024x332.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ingress-300x97.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/knative-serving-ingress-768x249.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-778087" class="wp-caption-text"&gt;Figure 4: The new knative-serving-ingress namespace.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We&amp;#8217;ve installed the OpenShift Operators and created the namespaces and the Knative Serving instance to manage our serverless workloads. We&amp;#8217;re now ready to create the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; resources that we&amp;#8217;ll need to run the continuous integration pipeline.&lt;/p&gt; &lt;h2&gt;Configure the Tekton tasks and pipeline&lt;/h2&gt; &lt;p&gt;When you install the OpenShift Pipelines Operator, it comes with an out-of-the-box set of cluster tasks that you can use to build your pipeline. In some situations, you will need other tasks to execute specific functionality. You can easily create these tasks in Tekton. You can also search the &lt;a target="_blank" rel="nofollow" href="https://hub-preview.tekton.dev/"&gt;Tekton Hub&lt;/a&gt; for reusable tasks and pipelines that are ready to be consumed.&lt;/p&gt; &lt;p&gt;For our pipeline, we will use one task from the Tekton Hub and two custom tasks. To make these tasks available to our pipeline, we&amp;#8217;ll need to create them in the &lt;code&gt;cicd&lt;/code&gt; namespace. (Note that you can create &lt;code&gt;ClusterTask&lt;/code&gt;s if you think that you&amp;#8217;ll reuse them in different pipelines from different namespaces.) Run the following script to install the needed tasks and create the pipeline in the same namespace.&lt;/p&gt; &lt;pre&gt;$ ./add-tekton-customs.sh cicd ------------------------------ Installing buildah task from https://hub-preview.tekton.dev/ task.tekton.dev/buildah created Installing custom tasks task.tekton.dev/push-knative-manifest created task.tekton.dev/workspace-cleaner created Installing knative-pipeline pipeline.tekton.dev/knative-pipeline created &lt;/pre&gt; &lt;p&gt;Navigate to the OpenShift console and open the &lt;b&gt;Pipelines&lt;/b&gt; menu and project &lt;b&gt;cicd&lt;/b&gt;. You will discover your new tasks, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_780317" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tasks.png"&gt;&lt;img aria-describedby="caption-attachment-780317" class="wp-image-780317 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tasks-1024x429.png" alt="A screenshot of the Tekton tasks in the CICD namespace." width="640" height="268" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tasks-1024x429.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tasks-300x126.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tasks-768x322.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780317" class="wp-caption-text"&gt;Figure 5: New Tekton tasks in the cicd namespace.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 6 shows your new pipeline in the same namespace.&lt;/p&gt; &lt;div id="attachment_780287" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/pipeline.png"&gt;&lt;img aria-describedby="caption-attachment-780287" class="wp-image-780287 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/pipeline-1024x430.png" alt="A screenshot of the Tekton pipeline in the CICD namespace." width="640" height="269" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/pipeline-1024x430.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/pipeline-300x126.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/pipeline-768x323.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780287" class="wp-caption-text"&gt;Figure 6: The Tekton pipeline in the cicd namespace.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Tekton workspaces&lt;/h3&gt; &lt;p&gt;Some of our tasks in the pipeline require either loading certain configurations from &lt;code&gt;ConfigMap&lt;/code&gt;s or storing the state of the resulting execution to be shared with other tasks. For instance, the Maven task requires that we include a specific &lt;code&gt;settings.xml&lt;/code&gt; in a &lt;code&gt;ConfigMap&lt;/code&gt;. On the other hand, the first task fetches the application&amp;#8217;s source code repository. The Maven task, which follows, will need those files to build the application JAR. We&amp;#8217;re using an OpenShift &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/storage/understanding-persistent-storage.html#persistent-volumes_understanding-persistent-storage"&gt;PersistentVolume&lt;/a&gt; to share these source files.&lt;/p&gt; &lt;p&gt;Tekton provides the concept of &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline/blob/master/docs/workspaces.md#workspaces"&gt;workspaces&lt;/a&gt; for these purposes. Run the following script to add a set of &lt;code&gt;ConfigMap&lt;/code&gt;s and a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; to the &lt;code&gt;cicd&lt;/code&gt; namespace:&lt;/p&gt; &lt;pre&gt;$ ./add-tekton-workspaces.sh cicd ----------------------------------- Creating knative-kustomize-base ConfigMap with base kustomize files for Knative services configmap/knative-kustomize-base created Creating knative-kustomize-environment ConfigMap with environment dependent kustomize files configmap/knative-kustomize-environment created Creating maven ConfigMap with settings.xml configmap/maven created Creating PVC using default storage class persistentvolumeclaim/source-pvc created &lt;/pre&gt; &lt;p&gt;Notice that this script creates a &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/storage/understanding-persistent-storage.html#pvc-storage-class_understanding-persistent-storage"&gt;PersistentVolumeClaim&lt;/a&gt; with no &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/storage/understanding-persistent-storage.html#pvc-storage-class_understanding-persistent-storage"&gt;StorageClass&lt;/a&gt; defined. Unless you choose to specify one, the default &lt;code&gt;StorageClass&lt;/code&gt; will be used. Feel free to uncomment any lines in the provided script to fit your needs.&lt;/p&gt; &lt;h2&gt;The demo application&lt;/h2&gt; &lt;p&gt;Until now, I&amp;#8217;ve said almost nothing about the demo application. The application is based on &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/get-started/"&gt;Quarkus&lt;/a&gt;, which is a perfect match for serverless applications due to its fast boot time and low memory consumption. The application itself is a simple &amp;#8220;Hello, world&amp;#8221; REST API that greets users when the &lt;code&gt;/hello&lt;/code&gt; URI is hit.&lt;/p&gt; &lt;p&gt;The application uses the &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/kubernetes-config"&gt;kubernetes-config&lt;/a&gt; extension to facilitate the consumption of &lt;code&gt;ConfigMap&lt;/code&gt;s and &lt;code&gt;Secrets&lt;/code&gt; in Kubernetes. The &amp;#8220;Hello, world&amp;#8221; application reads a list of &lt;code&gt;ConfigMap&lt;/code&gt;s, which gives us the chance to manage configuration at different levels, overriding duplicated properties.&lt;/p&gt; &lt;p&gt;Figure 7 shows an extract of the &lt;a target="_blank" rel="nofollow" href="https://github.com/dsanchor/quarkus-hello-world/blob/c076ee940b1f1d9576b7af3250bbbd7114e82263/src/main/resources/application.yaml#L18"&gt;application.yaml&lt;/a&gt; that defines the list of &lt;code&gt;ConfigMap&lt;/code&gt;s.&lt;/p&gt; &lt;div id="attachment_782297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/configmap-list-vsc.png"&gt;&lt;img aria-describedby="caption-attachment-782297" class="wp-image-782297" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/configmap-list-vsc.png" alt="A screenshot of the YAML file in the console." width="640" height="211" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/configmap-list-vsc.png 923w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/configmap-list-vsc-300x99.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/configmap-list-vsc-768x254.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-782297" class="wp-caption-text"&gt;Figure 7: Application YAML with the list of ConfigMaps.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can find the complete application source code in the &lt;a target="_blank" rel="nofollow" href="https://github.com/dsanchor/quarkus-hello-world.git"&gt;GitHub repository for this article&lt;/a&gt;. Note that the pipeline also initializes and continuously updates a different repository that contains, in a declarative way, all of the Kubernetes manifest for our application deployments and configurations. Later in the article, we&amp;#8217;ll use &lt;a target="_blank" rel="nofollow" href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; to declaratively customize the application configuration and deployment.&lt;/p&gt; &lt;h2&gt;Create your own repository&lt;/h2&gt; &lt;p&gt;At this stage, you must &lt;a target="_blank" rel="nofollow" href="https://docs.github.com/en/github/getting-started-with-github/create-a-repo"&gt;create a GitHub repository&lt;/a&gt; that you will use to store the customization files required for the demonstration. My repository is named &lt;code&gt;quarkus-hello-world-deployment&lt;/code&gt;, and I&amp;#8217;ll use that name to reference the repository in the upcoming scripts. You can use the same name or a different one for your repository.&lt;/p&gt; &lt;p&gt;After you have created and named the repository, leave it empty and initialized.&lt;/p&gt; &lt;p&gt;In order to allow the Tekton pipeline to push changes into the new repository, you will have to provide a valid set of GitHub credentials. You&amp;#8217;ll store the credentials in a &lt;code&gt;Secret&lt;/code&gt; and link them to the &lt;code&gt;ServiceAccount&lt;/code&gt; pipeline, which was automatically created in the &lt;code&gt;cicd&lt;/code&gt; namespace.&lt;/p&gt; &lt;p&gt;Execute the following script:&lt;/p&gt; &lt;pre&gt;$ ./add-github-credentials.sh cicd &lt;strong&gt;YOUR_GITHUB_USER YOUR_GITHUB_PASSWORD&lt;/strong&gt; --------------------------------------------------------------------------- Creating secret with github credentials for user dsanchor secret/github-credentials created Linking pipeline sa in namespace cicd with your github credentials serviceaccount/pipeline patched &lt;/pre&gt; &lt;h2&gt;A manual pipeline run&lt;/h2&gt; &lt;p&gt;We are now ready to manually test the pipeline&amp;#8217;s execution and see the results. The pipeline workflow includes a webhook setup that triggers the pipeline automatically. We&amp;#8217;ll leave that part for the end of this article (in Part 2); for now, we&amp;#8217;ll just test the workflow by triggering the pipeline manually.&lt;/p&gt; &lt;p&gt;I&amp;#8217;ve provided two options to trigger the pipeline manually:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a pipeline run from a YAML file.&lt;/li&gt; &lt;li&gt;Start the pipeline using the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/cli"&gt;Tekton CLI: tkn&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In both cases, we&amp;#8217;ll use a given &lt;code&gt;commit&lt;/code&gt; from the application repository. Also, we need to provide the repository that keeps all of our config and deployment manifests. In the script below, &lt;a target="_blank" rel="nofollow" href="https://github.com/dsanchor/quarkus-hello-world-deployment.git"&gt;I reference my deployment repository&lt;/a&gt;—you should replace that reference with the name of your repository. When you are ready, execute the following:&lt;/p&gt; &lt;pre&gt;$ cat tekton/pipelines/knative-pipeline-run.yaml | \   SOURCE_REPO=https://github.com/dsanchor/quarkus-hello-world.git \ COMMIT=9ce90240f96a9906b59225fec16d830ab4f3fe12 \ SHORT_COMMIT=9ce9024 \ DEPLOYMENT_REPO=https://github.com/dsanchor/quarkus-hello-world-deployment.git \   IMAGES_NS=cicd envsubst | \ oc create -f - -n cicd ------------------------------------------------------------------------------------------ pipelinerun.tekton.dev/knative-pipeline-run-54kpq created &lt;/pre&gt; &lt;p&gt;If you prefer to, you can start the pipeline using the &lt;em&gt;tkn&lt;/em&gt; CLI:&lt;/p&gt; &lt;pre&gt;$ tkn pipeline start knative-pipeline -p application=quarkus-hello-world \ -p source-repo-url=https://github.com/dsanchor/quarkus-hello-world.git \ -p source-revision=9ce90240f96a9906b59225fec16d830ab4f3fe12 \ -p short-source-revision=9ce9024 \ -p deployment-repo-url=https://github.com/dsanchor/quarkus-hello-world-deployment.git \ -p deployment-revision=master \ -p dockerfile=./src/main/docker/Dockerfile.jvm \ -p image-registry=image-registry.openshift-image-registry.svc.cluster.local:5000 \ -p image-repository=cicd \ -w name=source,claimName=source-pvc \ -w name=maven-settings,config=maven \ -w name=knative-kustomize-base,config=knative-kustomize-base \ -w name=knative-kustomize-environment,config=knative-kustomize-environment \ -n cicd &lt;/pre&gt; &lt;p&gt;Another option is to trigger the pipeline from the OpenShift console.&lt;/p&gt; &lt;h3&gt;Monitor the pipeline&amp;#8217;s execution&lt;/h3&gt; &lt;p&gt;To check the execution progress, visit the &lt;b&gt;Pipeline Runs&lt;/b&gt; dashboard in the OpenShift console, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_780397" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun.png"&gt;&lt;img aria-describedby="caption-attachment-780397" class="wp-image-780397 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-1024x187.png" alt="A screenshot of the Pipeline Runs dashboard." width="640" height="117" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-1024x187.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-300x55.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-768x140.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780397" class="wp-caption-text"&gt;Figure 8: Use the Pipeline Runs dashboard to check the execution progress.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you want to see all the details of each pipeline task, click in the name of the pipeline run. You will get the logs for each task, as shown in Figure 9:&lt;/p&gt; &lt;div id="attachment_780417" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-detail.png"&gt;&lt;img aria-describedby="caption-attachment-780417" class="wp-image-780417 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-detail-1024x584.png" alt="A screenshot of the pipeline task logs." width="640" height="365" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-detail-1024x584.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-detail-300x171.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/prun-detail-768x438.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780417" class="wp-caption-text"&gt;Figure 9: View the logs for each pipeline task.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you trigger the pipeline with exactly the same parameters twice (for instance, using both examples that I have provided) you will see that the second run fails when pushing the &lt;i&gt;Kustomization&lt;/i&gt; manifests. The failure happens because there is nothing new to commit—awesome!&lt;/p&gt; &lt;h3&gt;Outcomes of the pipeline execution&lt;/h3&gt; &lt;p&gt;The diagram in Figure 10 shows what we&amp;#8217;ve achieved so far:&lt;/p&gt; &lt;div id="attachment_790057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ci-manual-knative-2.png"&gt;&lt;img aria-describedby="caption-attachment-790057" class="wp-image-790057" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ci-manual-knative-2.png" alt="A diagram of the continuous integration workflow so far." width="640" height="390" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ci-manual-knative-2.png 836w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ci-manual-knative-2-300x183.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ci-manual-knative-2-768x469.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-790057" class="wp-caption-text"&gt;Figure 10: The CI/CD workflow in progress.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Note that we replaced the steps related to &amp;#8220;Push code&amp;#8221; and &amp;#8220;repository webhook&amp;#8221; with a manual pipeline trigger based on a given commit ID.&lt;/p&gt; &lt;p&gt;At this point, we&amp;#8217;ve pushed a new image to the OpenShift internal registry. We&amp;#8217;ve also initialized the repository that will contain all of the config and deployment manifests, along with all of the Kubernetes manifests that are required to run the first version of our serverless application.&lt;/p&gt; &lt;h2&gt;Reviewing the deployment repository structure&lt;/h2&gt; &lt;p&gt;Now is a good time to review the structure of the deployment repository and what will eventually be the final manifests that we&amp;#8217;ll generate with &lt;a target="_blank" rel="nofollow" href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;. If you are not familiar with Kustomize and its capabilities, feel free to learn more about it. Understanding Kustomize could help you to better understand the structure of the repository.&lt;/p&gt; &lt;p&gt;Update your deployment repository (&lt;code&gt;git pull&lt;/code&gt;) and you should see similar output to this:&lt;/p&gt; &lt;pre&gt;├── &lt;strong&gt;base&lt;/strong&gt; │   ├── global-ops-configmap.yaml │   ├── kservice.yaml │   └── kustomization.yaml ├── &lt;strong&gt;development&lt;/strong&gt; │   ├── env-ops-configmap.yaml │   ├── kustomization.yaml │   ├── r9ce9024 │   │   ├── configmap.yaml │   │   ├── revision-patch.yaml │   │   └── routing-patch.yaml │   └── traffic-routing.yaml ├── &lt;strong&gt;production&lt;/strong&gt; │   ├── env-ops-configmap.yaml │   ├── kustomization-r9ce9024.yaml │   ├── r9ce9024 │   │   ├── configmap.yaml │   │   ├── revision-patch.yaml │   │   └── routing-patch.yaml │   └── traffic-routing.yaml ├── README.md └── &lt;strong&gt;staging&lt;/strong&gt; ├── env-ops-configmap.yaml ├── kustomization-r9ce9024.yaml ├── r9ce9024 │   ├── configmap.yaml │   ├── revision-patch.yaml │   └── routing-patch.yaml └── traffic-routing.yaml &lt;/pre&gt; &lt;p&gt;For simplicity, I will only focus on the &lt;code&gt;base&lt;/code&gt; and &lt;code&gt;development&lt;/code&gt; folders for now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;base&lt;/code&gt; folder has all of the shared resources between the three environments. It holds the basic structure of a &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0/#service"&gt;Knative service&lt;/a&gt; and a global config map.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;development&lt;/code&gt; folder contains the overlays to complete the &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0/#service"&gt;Knative service&lt;/a&gt; manifest generation for a given application version (an example is the &lt;code&gt;r9ce9024&lt;/code&gt; folder) and two config maps that are related to the environment and developer configuration levels or ownership. The one under the revision folder has been copied from the application source code, letting the developer provide a set of configuration properties for the application.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We are taking advantage of the simplicity of Knative services to define independent &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0/#route"&gt;routes&lt;/a&gt; for each &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0/#revision"&gt;service revision&lt;/a&gt; and to &lt;a target="_blank" rel="nofollow" href="https://knative.dev/docs/serving/samples/traffic-splitting/"&gt;split traffic between revisions&lt;/a&gt;. Thus, the &lt;code&gt;traffic-routing.yaml&lt;/code&gt; and the &lt;code&gt;routing-patch.yaml&lt;/code&gt; form the final traffic-routing section of a Knative service.&lt;/p&gt; &lt;p&gt;Each time a new revision is available in &lt;code&gt;development&lt;/code&gt;, an independent route is created for it, to ensure that it is accessible for testing. The main route remains the same (for instance, targeting the other two previous revisions). We achieve this behavior by not modifying the main &lt;code&gt;traffic-routing.yaml&lt;/code&gt; automatically from the pipeline but only adding the new route (&lt;code&gt;routing-patch.yaml&lt;/code&gt;) for the new revision.&lt;/p&gt; &lt;p&gt;These details will be easier to understand when we run additional tests in Part 2. For now, just note a significant difference between the &lt;code&gt;staging&lt;/code&gt; and &lt;code&gt;production&lt;/code&gt; namespaces and &lt;code&gt;development&lt;/code&gt;: The CI pipeline does not create a &lt;code&gt;kustomization.yaml&lt;/code&gt; file (with that exact name) for them. There will always be one with an additional revision prefix: &lt;code&gt;kustomization-r9ce9024.yaml&lt;/code&gt;. Those changes will not be considered during the synchronization process unless this new revision is referenced in the &lt;code&gt;kustomization.yaml&lt;/code&gt;. A manual action is required to make the changes visible to Kustomize.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The point of the file-name difference is to differentiate the demonstration: I wanted those two environments to behave differently so that they would require someone to approve the changes. Renaming the file is a simple approach to approval that does not overcomplicate the demonstration. I would prefer to create a different branch for every new revision, then generate a pull request once it&amp;#8217;s ready to be promoted.&lt;/p&gt; &lt;h2&gt;Kustomize: Put all the pieces together&lt;/h2&gt; &lt;p&gt;We&amp;#8217;ve reviewed the content and structure of the deployment repository, but we still don&amp;#8217;t have the final composition of the Knative service and &lt;code&gt;ConfigMap&lt;/code&gt;s. The following script uses &lt;code&gt;kustomize&lt;/code&gt; to build the final manifests so that we can see how they look:&lt;/p&gt; &lt;pre&gt;$ kustomize build development ------------------------------ apiVersion: v1 kind: ConfigMap metadata: name: env-ops-quarkus-hello-world --- apiVersion: v1 kind: ConfigMap metadata: name: global-ops-quarkus-hello-world --- apiVersion: v1 data: application.yaml: |- message: hola environment: name: dev kind: ConfigMap metadata: name: quarkus-hello-world --- apiVersion: serving.knative.dev/v1 kind: Service metadata: name: quarkus-hello-world spec: template: metadata: name: quarkus-hello-world-r9ce9024 spec: containers: - image: image-registry.openshift-image-registry.svc.cluster.local:5000/cicd/quarkus-hello-world:9ce90240f96a9906b59225fec16d830ab4f3fe12 livenessProbe: httpGet: path: /health/live readinessProbe: httpGet: path: /health/ready traffic: - percent: 100 revisionName: quarkus-hello-world-r9ce9024 - revisionName: quarkus-hello-world-r9ce9024 tag: r9ce9024 &lt;/pre&gt; &lt;h2&gt;Conclusion for Part 1&lt;/h2&gt; &lt;p&gt;At this point, we could apply our set of objects into the &lt;code&gt;development&lt;/code&gt; namespace to get a serverless application running, but we don&amp;#8217;t want to do the deployment step manually. In the second half of this article, I will show you how to integrate &lt;a target="_blank" rel="nofollow" href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt; into the CI/CD pipeline that we&amp;#8217;ve developed so far.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#38;linkname=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F10%2F01%2Fbuilding-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1%2F&amp;#038;title=Building%20modern%20CI%2FCD%20workflows%20for%20serverless%20applications%20with%20Red%20Hat%20OpenShift%20Pipelines%20and%20Argo%20CD%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2020/10/01/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1/" data-a2a-title="Building modern CI/CD workflows for serverless applications with Red Hat OpenShift Pipelines and Argo CD, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/10/01/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1/"&gt;Building modern CI/CD workflows for serverless applications with Red Hat OpenShift Pipelines and Argo CD, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/eafqH_hG3wo" height="1" width="1" alt=""/&gt;</content><summary>A recent article, The present and future of CI/CD with GitOps on Red Hat OpenShift, proposed Tekton as a framework for cloud-native CI/CD pipelines, and Argo CD as its perfect partner for GitOps. GitOps practices support continuous delivery in hybrid, multi-cluster Kubernetes environments. In this two-part article, we’ll build a CI/CD workflow that demonstrates the potential of combining Tekton an...</summary><dc:creator>David Sancho</dc:creator><dc:date>2020-10-01T07:00:27Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/10/01/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1/</feedburner:origLink></entry><entry><title>Payments Architecture - Anti-money Laundering Example</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/I-3jDDAa8DY/payments-architecture-anti-money-laundering-example.html" /><category term="Architecture Blueprints" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="Decision Manager" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="JBossAMQ" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Process Automation Manager" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-payments_architecture_anti_money_laundering_example</id><updated>2020-10-01T08:43:59Z</updated><published>2020-10-01T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s1600/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"&gt;&lt;img alt="anti-money laundering" border="0" data-original-height="1067" data-original-width="1600" height="213" src="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s320/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="font-size: 12.8px; text-align: center;"&gt;Part 4 - Anti-money laundering&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Cloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service.&lt;br /&gt;&lt;br /&gt;So far we've presented research-based architectural blueprints of&amp;nbsp;&lt;a href="http://www.schabell.org/2018/11/integration-key-to-customer-experience-introduction.html" target="_blank"&gt;omnichannel customer experience&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.schabell.org/2020/01/integrating-saas-applications-an-introduction.html" target="_blank"&gt;integrating with SaaS applications&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://www.schabell.org/2020/05/cloud-native-development-a-blueprint.html" target="_blank"&gt;cloud-native development solutions&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In&amp;nbsp;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html" target="_blank"&gt;the previous article&lt;/a&gt;&amp;nbsp;in this series we walked through the immediate payments physical architecture.&lt;br /&gt;&lt;br /&gt;In this article we're diving into the anti-money laundering (AML) physical architecture, one based on successful customer solutions.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;Blueprints&lt;/h3&gt;&lt;div&gt;As a reminder, the architectural details covered here are base on real customer integration solutions using open source technologies.&lt;br /&gt;&lt;br /&gt;The example scenario presented here is a generic common blueprint that was uncovered researching customer solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details.&lt;br /&gt;&lt;br /&gt;This section covers the visual representations as presented. There are many ways to represent each element in this architectural blueprint, but we've chosen icons, text and colors that I hope are going to make it all easy to absorb. Feel free to post comments at the bottom of this post, or&amp;nbsp;&lt;a href="https://www.schabell.org/p/contact.html" target="_blank"&gt;contact us&lt;/a&gt;&amp;nbsp;directly with your feedback.&lt;br /&gt;&lt;br /&gt;Now let's take a look at the details in this blueprint and outline the example.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3 style="text-align: left;"&gt;Anti-money laundering&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;The example blueprint shown on the right entitled&amp;nbsp;&lt;i&gt;Anti-money laundering (AML) data example&lt;/i&gt;&amp;nbsp;outlines the solution in a physical architecture. Note that this diagram is focusing on the highest level of the AML solution and the element groupings that apply to this process.&lt;br /&gt;&lt;br /&gt;When you look at &lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html" target="_blank"&gt;our previous article &lt;/a&gt;where the immediate payments architecture was laid out as an overview, but if you look closely you'll notice one of the elements was called&amp;nbsp;&lt;i&gt;AML microservices. &lt;/i&gt;This section takes a closer look at what happens behind the scenes to a payments request that triggers a need for AML processing and detection.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-0cjVcI58GXM/X2nn49vh8rI/AAAAAAAAxj4/9K20_peOcpAed6tc8OjDvhaZnaEdAE9YQCNcBGAsYHQ/s1600/payments-anti-money-laundering-sd.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="anti-money laundering" border="0" data-original-height="900" data-original-width="1600" height="180" src="https://1.bp.blogspot.com/-0cjVcI58GXM/X2nn49vh8rI/AAAAAAAAxj4/9K20_peOcpAed6tc8OjDvhaZnaEdAE9YQCNcBGAsYHQ/s320/payments-anti-money-laundering-sd.png" title="" width="320" /&gt;&lt;/a&gt;In this example, starting from the top left corner, a user sends an event or message to execute a payment as an entry point. The users can be mobile, web, or any external device / application that acts as the entry point with the organizations payments architecture.&lt;br /&gt;&lt;br /&gt;This request to execute payments connects through API gateways (not depicted) to internal centralized p&lt;i&gt;ayments event streams&lt;/i&gt;. This element takes these streams and determines what selection or sub-selection of actions need to be taken. For this example, we'll proceed through this architecture as if AML processing is necessary.&lt;br /&gt;&lt;br /&gt;When an event triggers a compliance issue, such as suspected money laundering, the payment transaction(s) are analysed in &lt;i&gt;transaction scoring and labeling. &lt;/i&gt;It's a collection of rules fueled by data analytics that examine the suspect transactions, score them with a value, and tag them with labels before sending them on for specific evaluation as potential money laundering transactions.&lt;br /&gt;&lt;br /&gt;Feeding into&amp;nbsp;&lt;i&gt;transaction scoring and labeling&lt;/i&gt; are several elements of interest that provide a bit of data, analysis, and potential for applying artificial intelligence along with machine learning concepts. This starts with &lt;i&gt;know your customer (KYC) &lt;/i&gt;applications that are used to&amp;nbsp;verify the identity, suitability, and risks involved with maintaining a business relationship with each customer. Feeding the &lt;i&gt;KYC &lt;/i&gt;applications is data from &lt;i&gt;customers and transaction data.&amp;nbsp; &lt;/i&gt;Both of these elements are providing input to the &lt;i&gt;model training and serving&lt;/i&gt;&amp;nbsp;elements that generate models for scoring and labeling transactions.&lt;br /&gt;&lt;br /&gt;After modeling, scoring, and labeling suspect transactions, they're sent to &lt;i&gt;anti-money laundering rules&lt;/i&gt;, a collection of decision services that provide final evaluations and decision making on the suspect transactions. If it's determined the transactions are not money laundering actions, this outcome's fed back into the &lt;i&gt;payments event stream &lt;/i&gt;in a topic for further clearing processing (not shown in this diagram, see the previous article for clearing and routing architectural details).&lt;br /&gt;&lt;br /&gt;Finally, if the outcome is that the transactions are suspect, then they are passed off to the &lt;i&gt;malicious activity streams&lt;/i&gt;&amp;nbsp;element to start a topic of investigation. The actions taken are to open a case with the &lt;i&gt;case management &lt;/i&gt;element and to start a &lt;i&gt;suspicious activity reporting &lt;/i&gt;process. When investigations into the case and final reporting processing is completed, a reporting topic is used to funnel information back to the &lt;i&gt;malicious activity streams.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;The anti-money laundering architecture blueprint shown here is detailing the internal workings of the scoring, labeling, evanuation, processing, and reporting of suspect transactions. It's to be viewed as zooming in on the previous article's single element to provide more details on the physical architecture blueprint for this specific solution.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Project examples&lt;/h3&gt;&lt;div&gt;Sharing the process results for our payments blueprint is what this series is about, but there are project artifacts and diagrams that can also be shared with you the reader. We've pulled together an&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;examples repository&lt;/a&gt;&amp;nbsp;for all our architecture blueprint diagrams.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;The&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;Portfolio Architecture Examples&lt;/a&gt;&amp;nbsp;repository makes it possible to collect and share individual images from each diagram element as well as the entire project as a whole.&lt;/div&gt;&lt;div&gt;&lt;div style="text-align: right;"&gt;&lt;a href="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s1600/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="anti-money laundering" border="1" data-original-height="232" data-original-width="530" height="139" src="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s320/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" title="" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you scroll down to the file listings on the main page, you can locate all the example physical diagrams as shown on the right.&lt;br /&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;&lt;br /&gt;This is the collection associated with payments:&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;in this case there are multiple images you can click to view&lt;/li&gt;&lt;li&gt;a project file you can download to your local machine using the&amp;nbsp;&lt;i&gt;Download Diagram&lt;/i&gt;&amp;nbsp;link&lt;/li&gt;&lt;li&gt;a&amp;nbsp;&lt;i&gt;Load Diagram&lt;/i&gt;&amp;nbsp;link that you can&amp;nbsp;&lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-tooling/index.html?#/portfolio-architecture-examples/projects/schematic-diagrams-payments.drawio" target="_blank"&gt;click to automatically open the project diagrams&lt;/a&gt;&amp;nbsp;in the diagram tooling used in this blueprint (use private or incognito browser mode to avoid caching issues and a smoother tooling experience)&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;Give it a try and feel free to explore the collection of logical, schematic, detailed, solution, and community diagrams. This should allow you to get started much quicker than from scratch if you can kick-start a project with existing diagrams.&lt;br /&gt;&lt;br /&gt;Should you desire to start designing your own diagrams, please contribute the project file (ending in .drawio) by raising an issue with the file attached. We'd love to continue collecting these projects for others to use.&lt;br /&gt;&lt;br /&gt;Finally, there is a free online&amp;nbsp;&lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-workshops" target="_blank"&gt;beginners guide workshop&lt;/a&gt;&amp;nbsp;available focused on using the diagram tooling, please explore to learn tips and tricks from the experts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;What's next&lt;/h3&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;An overview of the series on the payments portfolio architecture blueprint can be found here:&lt;br /&gt;&lt;ol style="text-align: left;"&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-an-introduction.html" target="_blank"&gt;An introduction&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-common-elements.html" target="_blank"&gt;Common architecture elements&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html" target="_blank"&gt;Immediate payments example&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/10/payments-architecture-anti-money-laundering-example.html"&gt;Anti-money laundering example&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Fraud detection example&lt;/li&gt;&lt;li&gt;Financial calculations example&lt;/li&gt;&lt;/ol&gt;&lt;ol style="text-align: left;"&gt;&lt;/ol&gt;Catch up on any articles you missed by following one of the links above.&lt;br /&gt;&lt;br /&gt;Next in this series, taking a look at the&amp;nbsp;generic&amp;nbsp;&lt;i&gt;fraud detection example&lt;/i&gt;&amp;nbsp;in a cloud-native architecture focused on payment processing.&lt;br /&gt;&lt;br /&gt;(Article co-authored by&amp;nbsp;&lt;a href="https://www.linkedin.com/in/ramonv/?originalSubdomain=uk" target="_blank"&gt;Ramon Villarreal&lt;/a&gt;)&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=6PTj8MCJpxw:S1ILhJD8_Ws:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=6PTj8MCJpxw:S1ILhJD8_Ws:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=6PTj8MCJpxw:S1ILhJD8_Ws:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=6PTj8MCJpxw:S1ILhJD8_Ws:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=6PTj8MCJpxw:S1ILhJD8_Ws:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/6PTj8MCJpxw" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/I-3jDDAa8DY" height="1" width="1" alt=""/&gt;</content><summary>Part 4 - Anti-money launderingCloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service. So far we've presented research-based architectural blueprints of omnichannel customer experience, integrating with SaaS applications, and cloud-native ...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-10-01T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/6PTj8MCJpxw/payments-architecture-anti-money-laundering-example.html</feedburner:origLink></entry><entry><title>AI software stack inspection with Thoth and TensorFlow</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lvnLRySjp0s/" /><category term="Big Data" scheme="searchisko:content:tags" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="data science" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="human machine interaction" scheme="searchisko:content:tags" /><category term="Jupyter notebook" scheme="searchisko:content:tags" /><category term="machine learning" scheme="searchisko:content:tags" /><category term="performance" scheme="searchisko:content:tags" /><category term="Project Thoth" scheme="searchisko:content:tags" /><category term="software stack analysis" scheme="searchisko:content:tags" /><category term="TensorFlow" scheme="searchisko:content:tags" /><author><name>Francesco Murdaca</name></author><id>searchisko:content:id:jbossorg_blog-ai_software_stack_inspection_with_thoth_and_tensorflow</id><updated>2020-09-30T08:02:04Z</updated><published>2020-09-30T08:02:04Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/core/blob/master/doc/ROADMAP.md#thoth-roadmap"&gt;Project Thoth&lt;/a&gt; develops &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; tools that enhance the day-to-day life of developers and data scientists. Thoth uses machine-generated knowledge to boost the performance, security, and quality of your applications using &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt; (AI) through &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning."&gt;reinforcement learning (RL)&lt;/a&gt;. This machine-learning approach is implemented in &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth adviser&lt;/a&gt; (if you want to know more, &lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=WEJ65Rvj3lc&amp;#38;t=1s"&gt;click here&lt;/a&gt;) and it is used by &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/integration.rst"&gt;Thoth integrations&lt;/a&gt; to provide the software stack based on &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos#using-custom-configuration-file-template"&gt;user inputs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, I introduce a case study—a recent inspection of a runtime issue when importing TensorFlow 2.1.0—to demonstrate the human-machine interaction between the Thoth team and Thoth components. By following the case study from start to finish, you will learn how Thoth gathers and analyzes some of the data to provide &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/justifications"&gt;advice&lt;/a&gt; to its users, including bots such as &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/kebechet#kebechet"&gt;Kebechet&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://github.com/AICoE/aicoe-ci"&gt;AI-backed continuous integration pipelines&lt;/a&gt;, and developers using &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/qeb-hwt"&gt;GitHub apps&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Both the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station"&gt;Thoth machinery&lt;/a&gt; and team rely on bots and automated pipelines running on Red Hat OpenShift. Thoth takes a variety of inputs to determine the correct advice:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/solver"&gt;Solver&lt;/a&gt;, which Thoth uses to discover if something can be installed in a particular runtime environment, such as &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8 with Python 3.6.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/tree/master/notebooks/thoth-security-dataset#thoth-security-datasets"&gt;Security indicators&lt;/a&gt; that uncover vulnerabilities of a different nature, which can be applied to security advice.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/mi"&gt;Project meta information&lt;/a&gt;, such as project-maintenance status or development-process behavior that affects the overall project.&lt;/li&gt; &lt;li&gt;Inspections, which Thoth uses to discover code quality issues or performance across packages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article focuses on inspections. I will show you the results from an automated software stack inspection run through &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;&amp;#8216;s &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/dependency_monkey.rst"&gt;Dependency Monkey&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api#amun-service"&gt;Amun&lt;/a&gt; components. Thoth uses automated inspections to introduce new advice about software stacks for Thoth users. Another way to integrate advice could be via automated pipelines that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Boost performance&lt;/li&gt; &lt;li&gt;Optimize &lt;a href="https://developers.redhat.com/blog/category/machine-learning/"&gt;machine learning&lt;/a&gt; (ML) model inference&lt;/li&gt; &lt;li&gt;Ensure that there are no failures during the model runtime (for example, during inference)&lt;/li&gt; &lt;li&gt;Avoid using software stacks that does not guarantee security.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Thoth components: Amun and Dependency Monkey&lt;/h2&gt; &lt;p&gt;Given the list of packages that should be installed and the hardware requested to run the application, &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api#amun-service"&gt;Amun&lt;/a&gt; executes the requested application stack in the requested environment. Amun acts as an execution engine for Thoth. Applications are then built and tested using &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance"&gt;Thoth Performance Indicators (PI)&lt;/a&gt;. See Amun&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api/blob/master/README.rst"&gt;README documentation&lt;/a&gt; for more information about this service.&lt;/p&gt; &lt;p&gt;Another Thoth component, &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/dependency_monkey.rst"&gt;Dependency Monkey&lt;/a&gt;, can be used to schedule Amun. Dependency Monkey was designed to automate the evaluation of certain aspects of a software stack, such as code quality or performance. Therefore, it aims to automatically verify software stacks and aggregate relevant observations.&lt;/p&gt; &lt;p&gt;From these two components, the Thoth team created &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset"&gt;Thoth Performance Datasets&lt;/a&gt;, which contains observations about performance for software stacks. For example, Thoth Performance Datasets could use &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/conv2d.py"&gt;PIconv2d&lt;/a&gt; to obtain performance data for different application types (such as machine learning) and code quality. It could then use a performance indicator like &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; to discover errors during an application run.&lt;/p&gt; &lt;h2&gt;Transparent and reproducible datasets&lt;/h2&gt; &lt;p&gt;In the spirit of open source, the Thoth team wants to guarantee that the datasets and knowledge that we collect and use are transparent and reproducible. Machine learning models, such as the reinforcement learning model leveraged by &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth Adviser&lt;/a&gt;, should be as transparent as the datasets they are working on.&lt;/p&gt; &lt;p&gt;For transparency, we&amp;#8217;ve introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets#thoth-datasets"&gt;Thoth Datasets&lt;/a&gt;, where we share the notebooks that we used to analyze a data collection and all of the results. We encourage anyone interested in the topic to use Thoth Datasets to verify our findings or for other purposes.&lt;/p&gt; &lt;p&gt;For reproducibility, we&amp;#8217;ve introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo"&gt;Dependency Monkey Zoo&lt;/a&gt;, where we collect all of the specifications used to run an analysis. Having all of the specs in one place allows us to reproduce the results of a study. Anyone can use the specs to perform similar studies in different environments for comparison.&lt;/p&gt; &lt;h2&gt;Case study: Automated software stack inspection for TensorFlow 2.1.0&lt;/h2&gt; &lt;p&gt;For this case study, we will use Thoth&amp;#8217;s Amun and Dependency Monkey components to automatically produce data. We&amp;#8217;ll then introduce reusable &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/templates/Amun%20Inspection%20Analysis%20Template.ipynb"&gt;Jupyter notebook templates&lt;/a&gt; to extract specific information from the datasets. Finally, we&amp;#8217;ll create new advice based on the results.&lt;/p&gt; &lt;p&gt;The human side of this human-machine interaction focuses on assessing the quality of the results and formulating the advice. The rest of the process is machine-automated. Automation makes the process easy to repeat to produce a new source of information for analysis.&lt;/p&gt; &lt;p&gt;In the next sections, I introduce the initial problem, then describe the analysis performed and the resulting new advice for Thoth users.&lt;/p&gt; &lt;h2&gt;Initial request&lt;/h2&gt; &lt;p&gt;Our goal with this inspection is to analyze build- and runtime failures when importing TensorFlow 2.1.0 and use these to derive observations about the quality of the software stack.&lt;/p&gt; &lt;p&gt;For this analysis, Dependency Monkey sampled the state space of all of the possible &lt;code&gt;TensorFlow==2.1.0&lt;/code&gt; stacks (from upstream builds). For inspection purposes, we built and ran the application using the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/matmul.py"&gt;PiMatmul&lt;/a&gt; performance indicator.&lt;/p&gt; &lt;p&gt;The sections below detail the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo/tree/master/tensorflow/inspection-2020-09-04"&gt;Dependency Monkey inspection results&lt;/a&gt; and the resulting &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/notebooks/issues/70"&gt;analysis&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The first analysis&lt;/h2&gt; &lt;p&gt;From the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset/PerformanceTensorFlow2.1.0SoftwareStackCombinations.ipynb"&gt;software stack analysis&lt;/a&gt; of inspection results, we discovered that TensorFlow 2.1.0 was giving errors during approximately 50% of inspections during a run. The error is shown in the following output from the Jupyter Notebook:&lt;/p&gt; &lt;pre&gt;'2020-09-05 07:14:36.333589: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libnvinfer.so.6\'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory 2020-09-05 07:14:36.333811: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libnvinfer_plugin.so.6\'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory 2020-09-05 07:14:36.333844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /opt/app-root/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters /opt/app-root/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.5) or chardet (2.3.0) doesn\'t match a supported version! RequestsDependencyWarning) Traceback (most recent call last): File "/home/amun/script", line 14, in &amp;#60;module&amp;#62; import tensorflow as tf File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 101, in &amp;#60;module&amp;#62; from tensorflow_core import * File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/__init__.py", line 40, in &amp;#60;module&amp;#62; from tensorflow.python.tools import module_util as _module_util File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 50, in __getattr__ module = self._load() File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 44, in _load\n module = _importlib.import_module(self.__name__) File "/opt/app-root/lib64/python3.6/importlib/__init__.py", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/__init__.py", line 95, in &amp;#60;module&amp;#62; from tensorflow.python import keras File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py", line 27, in &amp;#60;module&amp;#62; from tensorflow.python.keras import models File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py", line 27, in &amp;#60;module&amp;#62; from tensorflow.python.keras import models File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py", line 25, in &amp;#60;module&amp;#62; from tensorflow.python.keras.engine import network File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 46, in &amp;#60;module&amp;#62; from tensorflow.python.keras.saving import hdf5_format File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py", line 32, in &amp;#60;module&amp;#62; from tensorflow.python.keras.utils import conv_utils File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/conv_utils.py", line 22, in &amp;#60;module&amp;#62; from six.moves import range # pylint: disable=redefined-builtin ImportError: cannot import name \'range\''&lt;/pre&gt; &lt;p&gt;Specifically, we could see that some combinations of &lt;code&gt;six&lt;/code&gt; and &lt;code&gt;urllib3&lt;/code&gt; produced that error, as described in the following &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/notebooks/issues/70#issuecomment-688656575"&gt;output&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;============================================= urllib3 ============================================= In successfull inspections: ['urllib3-1.10.4-pypi-org' 'urllib3-1.16-pypi-org' 'urllib3-0.3-pypi-org' 'urllib3-1.21.1-pypi-org' 'urllib3-1.25.1-pypi-org' 'urllib3-1.25-pypi-org' 'urllib3-1.18.1-pypi-org' 'urllib3-1.24.1-pypi-org' 'urllib3-1.10.1-pypi-org' 'urllib3-1.10.3-pypi-org' 'urllib3-1.25.7-pypi-org' 'urllib3-1.10-pypi-org' 'urllib3-1.7.1-pypi-org' 'urllib3-1.13-pypi-org' 'urllib3-1.19.1-pypi-org' 'urllib3-1.11-pypi-org' 'urllib3-1.10.2-pypi-org' 'urllib3-1.15.1-pypi-org' 'urllib3-1.25.3-pypi-org' 'urllib3-1.13.1-pypi-org' 'urllib3-1.21-pypi-org' 'urllib3-1.17-pypi-org' 'urllib3-1.23-pypi-org'] In failed inspections: ['urllib3-1.5-pypi-org'] In failed inspections but not in successfull: {'urllib3-1.5-pypi-org'} In failed inspections and in successfull: set() ============================================= six ============================================= In successfull inspections: ['six-1.13.0-pypi-org' 'six-1.12.0-pypi-org'] In failed inspections: ['six-1.13.0-pypi-org' 'six-1.12.0-pypi-org'] In failed inspections but not in successfull: set() In failed inspections and in successfull: {'six-1.13.0-pypi-org', 'six-1.12.0-pypi-org'}&lt;/pre&gt; &lt;p&gt;Therefore, we discovered that &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/urllib3/"&gt;&lt;strong&gt;urllib3&lt;/strong&gt;&lt;/a&gt; library releases were the same across all failed inspections but not in any of the successful inspections, while &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/six/"&gt;&lt;strong&gt;six&lt;/strong&gt;&lt;/a&gt; library releases didn&amp;#8217;t show any differences between failed and successful once.&lt;/p&gt; &lt;h2&gt;The second analysis&lt;/h2&gt; &lt;p&gt;For our next step, we decided to run another analysis to restrict the cases. For this run, we used a newly created performance indicator called &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; as shown in Table 1.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 1: The PiImport performance indicator.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Dependency Monkey sampled the state space of all the possible &lt;code&gt;TensorFlow==2.1.0&lt;/code&gt; stacks (from upstream builds). The application was built and run using the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; performance indicator.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Specification  &lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo/tree/master/tensorflow/inspection-2020-09-08.1"&gt;Dependency Monkey specification&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Identify specific versions that fail to produce final &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/pull/1172"&gt;advice&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/issues/16"&gt;Issue&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Results of the second analysis&lt;/h2&gt; &lt;p&gt;From the new &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset/PerformanceTensorFlow2.1.0SoftwareStackCombinationsErrors.ipynb"&gt;analysis&lt;/a&gt;, we were able to identify all of the specific versions of &lt;code&gt;urllib3&lt;/code&gt; and &lt;code&gt;six&lt;/code&gt; that did not work together and that were causing issues during runtime. The output in Figure 1 shows the incompatible versions of the two packages.&lt;/p&gt; &lt;p&gt;dFigure 1: Identifying the incompatible versions of urllib3 and six that do not allow to run Tensorflow 2.1.0.&lt;/p&gt; &lt;h2&gt;The advice&lt;/h2&gt; &lt;p&gt;All of this backtracing led to an &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/pull/1172"&gt;adviser step&lt;/a&gt; called &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/thoth/adviser/steps/tensorflow_21_urllib3.py"&gt;TensorFlow21Urllib3Step&lt;/a&gt;. With this step, we can penalize software stacks containing the specific version of &lt;code&gt;urllib3&lt;/code&gt; that cause runtime issues when attempting to import TensorFlow 2.1.0. The following prediction, created by Thoth, results in a higher quality software stack for users.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 2: The TensorFlow21Urllib3Step adviser step.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Title&lt;/b&gt;&lt;/td&gt; &lt;td&gt;TensorFlow in version 2.1 can cause runtime errors when imported, caused by incompatibility between &lt;code&gt;urllib3&lt;/code&gt; and &lt;code&gt;six&lt;/code&gt; packages.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Issue description&lt;/b&gt;&lt;/td&gt; &lt;td&gt;Package &lt;code&gt;urllib3&lt;/code&gt; in some versions is shipped with a bundled version of &lt;code&gt;six&lt;/code&gt;, which has its own mechanism for imports and import context handling. Importing &lt;code&gt;urllib3&lt;/code&gt; in the TensorFlow codebase causes initialization of the bundled &lt;code&gt;six&lt;/code&gt; module, which collides with a subsequent import from unbundled &lt;code&gt;six&lt;/code&gt; modules.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;You can find the complete issue description, and the recommended resolution, &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/j/tf_21_urllib3.html"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#038;title=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" data-a2a-url="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/" data-a2a-title="AI software stack inspection with Thoth and TensorFlow"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/"&gt;AI software stack inspection with Thoth and TensorFlow&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lvnLRySjp0s" height="1" width="1" alt=""/&gt;</content><summary>Project Thoth develops open source tools that enhance the day-to-day life of developers and data scientists. Thoth uses machine-generated knowledge to boost the performance, security, and quality of your applications using artificial intelligence (AI) through reinforcement learning (RL). This machine-learning approach is implemented in Thoth adviser (if you want to know more, click here) and it is...</summary><dc:creator>Francesco Murdaca</dc:creator><dc:date>2020-09-30T08:02:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/</feedburner:origLink></entry><entry><title>This Week in JBoss - 30 September 2020</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6SrCuoxXhnk/weekly-2020-09-30.html" /><category term="Camel" scheme="searchisko:content:tags" /><category term="camelk" scheme="searchisko:content:tags" /><category term="devconf" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="jBPM" scheme="searchisko:content:tags" /><category term="Kafka" scheme="searchisko:content:tags" /><category term="OCP" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Paul Robinson</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_30_september_2020</id><updated>2020-09-30T00:00:00Z</updated><published>2020-09-30T00:00:00Z</published><content type="html">&lt;article class="" data-tags="jBPM, OCP, OpenShift, Camel, CamelK, DevConf, Kafka"&gt; &lt;h1&gt;This Week in JBoss - 30 September 2020&lt;/h1&gt; &lt;p&gt;It’s that time again where we round up the latest news from the community…&lt;/p&gt; &lt;p&gt;Are you looking to develop projects on your local machine, and push them on to a real OpenShift Container Platform, without having to worry about cloud hosting of your container platform? In this &lt;a href="https://www.schabell.org/2020/09/how-to-setup-openshift-container-platform-45.html"&gt;post&lt;/a&gt; Eric Schabell provides an easy way to get up and running with CodeReady containers on your own development machine. He’s automated most of the process through scripts, and guides you through the rest of the process. Eric also shows you how to drop into the Developer console in OCP providing a developer focused view that hides much of the plumbing that is not needed during development.&lt;/p&gt; &lt;p&gt;In this &lt;a href="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/"&gt;post&lt;/a&gt; Mary Cochran shows us a simple way to connect existing services together using CamelK via each service’s REST APIs. Mary gets deep into the details providing code examples and commands to get you up and running.&lt;/p&gt; &lt;p&gt;Eric Schabell continues his &lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-an-introduction.html"&gt;blog series&lt;/a&gt;, bringing you architectural blueprints for&amp;#160;cloud-native financial payment services. The blueprints are focused on proven interactions, messaging, processing, and integration patterns that you can put to use when building a cloud-native payment architecture. This week he covers the &lt;a href="https://www.schabell.org/2020/09/payments-architecture-common-elements.html"&gt;Common Architecture Elements&lt;/a&gt; that make up the architecture, and gets into some details with the &lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html"&gt;Immediate Payments Example&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As is normal for this time of year, the conference season is beginning to kick off. Albeit this year in a virtual arena. Undoubtedly this is an unfortunate situation for those that enjoy the in-person events. However, it’s great for accessibility and reach allowing many more people across to globe to attend. DevConf.US 2020 is one such event. It’s a free, Red Hat sponsored technology conference for community project and professional contributors to Free and Open Source technologies. Read more &lt;a href="https://www.schabell.org/2020/09/devconfus-2020-appdev-containerization-ask-the-experts.html"&gt;here&lt;/a&gt; where Eric describes the “AppDev &amp;#38; Containerization Ask the Experts” Panels that he and Kurt Stam will be moderating.&lt;/p&gt; &lt;p&gt;This week Kapil Shukla explains how to &lt;a href="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/"&gt;build a data streaming pipeline using Kafka Streams and Quarkus&lt;/a&gt;. In this post Kapil shows how data can be processed in real-time as and when it arrives, instead of being batch processed as was needed in the past.&lt;/p&gt; &lt;p&gt;In this &lt;a href="https://developers.redhat.com/blog/2020/09/22/troubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite/"&gt;Post&lt;/a&gt; Anton Giertli provides help with troubleshooting user task errors in Red Hat Process Automation Manager and Red Hat JBoss BPM Suite. If you are a developer working with user tasks and having trouble debugging them, this post might well help you out.&lt;/p&gt; &lt;p&gt;With the recent release of Apache Camel VS Code extension (0.0.27) comes several new language support features for Apache Camel. Read this &lt;a href="https://developers.redhat.com/blog/2020/09/18/new-language-support-features-in-apache-camel-vs-code-extension-0-0-27/"&gt;post&lt;/a&gt; to learn more.&lt;/p&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/paul-robinson.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Paul Robinson&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6SrCuoxXhnk" height="1" width="1" alt=""/&gt;</content><summary>This Week in JBoss - 30 September 2020 It’s that time again where we round up the latest news from the community… Are you looking to develop projects on your local machine, and push them on to a real OpenShift Container Platform, without having to worry about cloud hosting of your container platform? In this post Eric Schabell provides an easy way to get up and running with CodeReady containers on...</summary><dc:creator>Paul Robinson</dc:creator><dc:date>2020-09-30T00:00:00Z</dc:date><feedburner:origLink>https://www.jboss.org/posts/weekly-2020-09-30.html</feedburner:origLink></entry><entry><title>Quicker, easier GraphQL queries with Open Liberty 20.0.0.9</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CeJhWusLdiQ/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="graphql mutation" scheme="searchisko:content:tags" /><category term="graphql query" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="microprofile" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><category term="OpenLiberty" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Jakub Pomykala</name></author><id>searchisko:content:id:jbossorg_blog-quicker_easier_graphql_queries_with_open_liberty_20_0_0_9</id><updated>2020-09-29T07:00:48Z</updated><published>2020-09-29T07:00:48Z</published><content type="html">&lt;p&gt;Open Liberty 20.0.0.9 lets developers experiment with the type-safe SmallRye GraphQL Client API, and write and run GraphQL queries and mutations more easily with a built-in GraphiQL user interface (UI). This article introduces the new features and updates in Open Liberty 20.0.0.9:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#GraphQLAPIs"&gt;Experiment with a third-party GraphQL client API.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#GraphiQL"&gt;Use the built-in GraphiQL UI for faster queries and mutations.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#feedback"&gt;Give us your feedback!&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Run your apps using Open Liberty 20.0.0.9&lt;/h2&gt; &lt;p&gt;If you are using &lt;a target="_blank" rel="nofollow" href="https://openliberty.io//guides/maven-intro.html"&gt;Maven&lt;/a&gt;, use these coordinates to update to the newest version of Open Liberty:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.openliberty&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;openliberty-runtime&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;20.0.0.9&amp;#60;/version&amp;#62; &amp;#60;type&amp;#62;zip&amp;#60;/type&amp;#62; &amp;#60;/dependency&amp;#62; &lt;/pre&gt; &lt;p&gt;For &lt;a target="_blank" rel="nofollow" href="https://openliberty.io//guides/gradle-intro.html"&gt;Gradle&lt;/a&gt;, enter:&lt;/p&gt; &lt;pre&gt;dependencies { libertyRuntime group: 'io.openliberty', name: 'openliberty-runtime', version: '[20.0.0.9,)' } &lt;/pre&gt; &lt;p&gt;If you&amp;#8217;re using Docker, it&amp;#8217;s:&lt;/p&gt; &lt;pre&gt;FROM open-liberty &lt;/pre&gt; &lt;h2 id="GraphQLAPIs"&gt;Experiment with a third-party GraphQL client API&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/microprofile-graphql"&gt;MicroProfile GraphQL&lt;/a&gt; has only been available in Open Liberty for a few months, and it is already a hit. That said, there are a few ways that we want to improve it and make it more complete. One feature that we want to improve is the GraphQL client API. While the official client API is not expected until the next release of MicroProfile GraphQL, you can start experimenting now with the type-safe SmallRye GraphQL client API.&lt;/p&gt; &lt;p&gt;SmallRye is the underlying implementation of MicroProfile GraphQL. You can access its above-and-beyond-the-spec features by adding the &amp;#8220;third-party&amp;#8221; API type visibility to your application:&lt;/p&gt; &lt;pre&gt;&amp;#60;application name="MyGraphQLApp" location="MyGraphQLApp.war"&amp;#62; &amp;#60;classloader apiTypeVisibility="+third-party"/&amp;#62; &amp;#60;/application&amp;#62; &lt;/pre&gt; &lt;p&gt;This update lets you access SmallRye GraphQL APIs like the type-safe client. Note that these APIs might change in future releases because SmallRye is continuously evolving. For more information, please visit &lt;a target="_blank" rel="nofollow" href="https://github.com/smallrye/smallrye-graphql"&gt;SmallRye GraphQL project&lt;/a&gt;. For Open Liberty 20.0.0.9, we are using SmallRye GraphQL 1.0.7.&lt;/p&gt; &lt;h3&gt;Type-safe invocation for remote methods&lt;/h3&gt; &lt;p&gt;The SmallRye GraphQL client APIs are very similar to &lt;a href="https://developers.redhat.com/cheat-sheets/microprofile-rest-client"&gt;MicroProfile Rest Client&lt;/a&gt;, which uses an interface to invoke remote methods in a type-safe manner. For example, suppose we want a client that can invoke a query of all of the superheroes in a given location. We would create a query interface like this:&lt;/p&gt; &lt;pre&gt;@GraphQlClientApi interface SuperHeroesApi { List allHeroesIn(String location); } &lt;/pre&gt; &lt;p&gt;Where &lt;code&gt;SuperHero&lt;/code&gt; on the client-side looks like this:&lt;/p&gt; &lt;pre&gt;class SuperHero { private String name; private List superPowers; } &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;SuperHero&lt;/code&gt; entity might contain dozens of fields on the server-side, but if we&amp;#8217;re only interested in the hero&amp;#8217;s name and superpowers, then we only need those two fields in our client-side class. Now, we can invoke the query with code like this:&lt;/p&gt; &lt;pre&gt;SuperHeroesApi api = GraphQlClientBuilder.newBuilder().build(SuperHeroesApi.class); List heroesOfNewYork = api.allHeroesIn("NYC"); &lt;/pre&gt; &lt;p&gt;Remember that this client API is not official, but the official MicroProfile GraphQL 1.1 API will be based on it. Think of this as a preview.&lt;/p&gt; &lt;h2 id="GraphiQL"&gt;Use the built-in GraphiQL UI for faster queries and mutations&lt;/h2&gt; &lt;p&gt;Open Liberty now sports a built-in &lt;a target="_blank" rel="nofollow" href="https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md"&gt;GraphiQL&lt;/a&gt; user interface as shown in Figure 1. The new, web-based UI allows you to write and execute GraphQL queries and mutations in real-time with advanced editing features like command completion, query history, schema introspection, and so on.&lt;/p&gt; &lt;div id="attachment_786817" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00.png"&gt;&lt;img aria-describedby="caption-attachment-786817" class="wp-image-786817 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-1024x391.png" alt="Web UI open to edit a query, with hover-over text displayed." width="640" height="244" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-1024x391.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-300x115.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-768x293.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786817" class="wp-caption-text"&gt;Figure 1: Work faster with the GraphiQL web UI.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To enable the UI, you must first &lt;a target="_blank" rel="nofollow" href="https://openliberty.io/blog/2020/06/10/microprofile-graphql-open-liberty.html"&gt;write and deploy a MicroProfile GraphQL application&lt;/a&gt;. Then add this line to your &lt;code&gt;server.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&amp;#60;variable name="io.openliberty.enableGraphQLUI" value="true" /&amp;#62; &lt;/pre&gt; &lt;p&gt;You can use a web browser to access the UI, by merely opening your GraphQL application&amp;#8217;s context root and adding &lt;code&gt;/graphql-ui&lt;/code&gt;. As an example, suppose that we use the default port (9080) and our application is named&lt;code&gt;myGraphQLApp&lt;/code&gt;. In that case, we would access the UI at &lt;code&gt;http://localhost:9080/myGraphQLApp/graphql-ui&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This workaround was &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues/13201"&gt;resolved in issue #13201&lt;/a&gt;.&lt;/p&gt; &lt;h2 id="feedback"&gt;We want your feedback&lt;/h2&gt; &lt;p&gt;As an open source team, we love receiving feedback from Open Liberty users. A recent example is this comment, taken from Open Liberty &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues/13036"&gt;#13036&lt;/a&gt;): &amp;#8220;Hello, I am using microprofile-graphql on openliberty and everything goes well except for the exception whitelisting mechanism via microprofile config &amp;#8230;&amp;#8221;&lt;/p&gt; &lt;p&gt;Our MicroProfile GraphQL feature has only been generally available for a few months, so it&amp;#8217;s great to know that users are adopting it. We&amp;#8217;re also excited that some of you are already exploring the &amp;#8220;dark corners&amp;#8221; of exception handling and similar features.&lt;/p&gt; &lt;p&gt;While we dislike discovering that we let a bug slip through the cracks, we&amp;#8217;re eager to fix them when they do. If you find an issue or want to suggest an enhancement that would make your experience with Open Liberty better, please let us know. You can always reach us by &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues"&gt;opening an issue on GitHub&lt;/a&gt; or contacting us on Twitter at &lt;a target="_blank" rel="nofollow" href="https://twitter.com/OpenLibertyIO"&gt;@OpenLibertyIO&lt;/a&gt;. We&amp;#8217;re also available to chat online using &lt;a target="_blank" rel="nofollow" href="https://gitter.im/OpenLiberty/help"&gt;Gitter&lt;/a&gt; and on the &lt;a target="_blank" rel="nofollow" href="https://gitter.im/OpenLiberty/developer-experience"&gt;Open Liberty Developer Experience&lt;/a&gt; page.&lt;/p&gt; &lt;h2&gt;Try Open Liberty 20.0.0.8 in Red Hat Runtimes now&lt;/h2&gt; &lt;p&gt;Open Liberty is part of the Red Hat Runtimes offering and is available to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-runtimes"&gt;Red Hat Runtimes subscribers&lt;/a&gt;. To learn more about deploying Open Liberty applications to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, see our &lt;i&gt;&lt;a href="https://openliberty.io/guides/cloud-openshift.html" target="_blank" rel="nofollow noopener noreferrer"&gt;Open Liberty guide: Deploying microservices to OpenShift&lt;/a&gt;&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#038;title=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" data-a2a-url="https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/" data-a2a-title="Quicker, easier GraphQL queries with Open Liberty 20.0.0.9"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/"&gt;Quicker, easier GraphQL queries with Open Liberty 20.0.0.9&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CeJhWusLdiQ" height="1" width="1" alt=""/&gt;</content><summary>Open Liberty 20.0.0.9 lets developers experiment with the type-safe SmallRye GraphQL Client API, and write and run GraphQL queries and mutations more easily with a built-in GraphiQL user interface (UI). This article introduces the new features and updates in Open Liberty 20.0.0.9: Experiment with a third-party GraphQL client API. Use the built-in GraphiQL UI for faster queries and mutations. Give ...</summary><dc:creator>Jakub Pomykala</dc:creator><dc:date>2020-09-29T07:00:48Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/</feedburner:origLink></entry><entry><title>Call an existing REST service with Apache Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/TdDN8vPz2vY/" /><category term="application integration" scheme="searchisko:content:tags" /><category term="Camel K" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="jitpack" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="OpenAPI" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="operator" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><author><name>Mary Cochran</name></author><id>searchisko:content:id:jbossorg_blog-call_an_existing_rest_service_with_apache_camel_k</id><updated>2020-09-28T07:00:41Z</updated><published>2020-09-28T07:00:41Z</published><content type="html">&lt;p&gt;With the &lt;a href="https://developers.redhat.com/blog/2020/06/18/camel-k-1-0-the-serverless-integration-platform-goes-ga/"&gt;release of Apache Camel K&lt;/a&gt;, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What happens when the cafe wants to allow customers to use a delivery service like GrubHub? You would only need to introduce a single &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/"&gt;Camel K&lt;/a&gt; integration to connect the cafe and GrubHub systems.&lt;/p&gt; &lt;p&gt;In this article, I will show you how to create a &lt;a href="https://developers.redhat.com/videos/youtube/51x9BewGCYA"&gt;Camel K integration&lt;/a&gt; that calls an existing REST service and uses its existing data format. For the data format, I have a Maven project configured with &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; objects. Ideally, you would have this packaged and available in a Nexus repository. For the purpose of my demonstration, I utilized &lt;a target="_blank" rel="nofollow" href="https://jitpack.io/"&gt;JitPack&lt;/a&gt;, which lets me have my dependency available in a repository directly from my GitHub code. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/grubhub-cafe-core"&gt;GitHub repository associated with this demo&lt;/a&gt; for the data format code and directions for getting it into JitPack.&lt;br /&gt; &lt;span id="more-760597"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;In order to follow the demonstration, you will need the following installed in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; command-line tools&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/apache/camel-k/releases"&gt;Camel K client 1.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.4 cluster&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Create the Camel K route&lt;/h2&gt; &lt;p&gt;First, we create the Camel K route file, which I have named &lt;code&gt;RestWithUndertow.java&lt;/code&gt;. Here, we open the file and create the class structure:&lt;/p&gt; &lt;pre&gt;public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { } }&lt;/pre&gt; &lt;p&gt;Next, we create the REST endpoint, and we also create the data formats that we will use. In this case, we&amp;#8217;ll receive the REST request as a &lt;code&gt;GrubHubOrder&lt;/code&gt;. We&amp;#8217;ll then transform it to a &lt;code&gt;CreateOrderCommand&lt;/code&gt;, which we&amp;#8217;ll send to the REST service that is already in use:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); } } &lt;/pre&gt; &lt;h2&gt;Create the data transformation&lt;/h2&gt; &lt;p&gt;Now we can create a method in the same file, which will assist with the data transformation from the &lt;code&gt;GrubHubOrder&lt;/code&gt; to the &lt;code&gt;CreateOrderCommand&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } &lt;/pre&gt; &lt;p&gt;Make sure that you add the following imports to the file, as well:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; &lt;/pre&gt; &lt;h2&gt;Call the existing service from your Camel K REST endpoint&lt;/h2&gt; &lt;p&gt;Now that we have a method to do the transformation, we can implement the rest of the Camel K REST endpoint and make it call the existing service. Add the following below the code that you have so far:&lt;/p&gt; &lt;pre&gt; from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) //transforms the java object into json .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") .to("http://?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); &lt;/pre&gt; &lt;p&gt;Note that this example includes plenty of logging to give visibility to what is being done. I have set the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt;, which allows us to ignore the &lt;code&gt;HTTP_URI&lt;/code&gt; incoming header and use the full URL that we&amp;#8217;ve specified. This is important when taking an incoming REST request that will call a new one. You can read more about &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/http-component.html"&gt;the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Save the route file&lt;/h2&gt; &lt;p&gt;Your complete Camel K route file should look something like this:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") //need to change url after knowing what the cafe-web url will be .to("http://sampleurl.com?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); } public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } }&lt;/pre&gt; &lt;p&gt;Save this file and get ready for the last step.&lt;/p&gt; &lt;h2&gt;Run the integration&lt;/h2&gt; &lt;p&gt;To run your integration, you will need to have the Camel K Operator installed and ensure that it has access to the dependencies in JitPack. Do the following to get your infrastructure ready for the integration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Log in to OpenShift using the &lt;code&gt;oc&lt;/code&gt; command line.&lt;/li&gt; &lt;li&gt;Install the Camel K Operator via the OpenShift OperatorHub. The default options are fine.&lt;/li&gt; &lt;li&gt;Ensure you have &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/cli/cli.html"&gt;Kamel CLI tooling &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;kamel&lt;/code&gt; tools and the following command to create an integration platform that provides access to JitPack: &lt;pre&gt;kamel install --olm=false --skip-cluster-setup --skip-operator-setup --maven-repository https://jitpack.io@id=jitpack@snapshots &lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you see that everything is ready in your OpenShift console (you can always enter &lt;code&gt;oc get pods&lt;/code&gt;to check), deploy the integration. Using your Kamel tools again, make sure that you are logged into OpenShift and on the appropriate project, and run the following command:&lt;/p&gt; &lt;pre&gt;kamel run --name=rest-with-undertow --dependency=camel-jackson --dependency=mvn:com.github.jeremyrdavis:quarkus-cafe-demo:1.5-SNAPSHOT --dependency=mvn:com.github.jeremyrdavis.quarkus-cafe-demo:grubhub-cafe-core:1.5-SNAPSHOT --dependency=camel-openapi-java RestWithUndertow.java &lt;/pre&gt; &lt;p&gt;This command ensures that your route starts with all of the appropriate dependencies. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/camel-k-grub-hub"&gt;GitHub repository for this article&lt;/a&gt; for a complete,  working version of this code and the service that it calls.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Camel K allows you to develop your integrations quickly and efficiently while keeping your footprint small. Even when you have some dependencies for your integrations you can utilize the Knative technology in Camel K to make your integrations less resource intensive and allow for quicker deployments.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#038;title=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/" data-a2a-title="Call an existing REST service with Apache Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/"&gt;Call an existing REST service with Apache Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/TdDN8vPz2vY" height="1" width="1" alt=""/&gt;</content><summary>With the release of Apache Camel K, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What happens when the cafe wants to allow customers to use a delivery service like Grub...</summary><dc:creator>Mary Cochran</dc:creator><dc:date>2020-09-28T07:00:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/</feedburner:origLink></entry><entry><title>Build a data streaming pipeline using Kafka Streams and Quarkus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6-PYLEG5zIc/" /><category term="data pipeline" scheme="searchisko:content:tags" /><category term="data streaming" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kafka streams" scheme="searchisko:content:tags" /><category term="kafka tutorial" scheme="searchisko:content:tags" /><category term="message processing" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="Spring Boot" scheme="searchisko:content:tags" /><category term="Stream Processing" scheme="searchisko:content:tags" /><author><name>Kapil Shukla</name></author><id>searchisko:content:id:jbossorg_blog-build_a_data_streaming_pipeline_using_kafka_streams_and_quarkus</id><updated>2020-09-28T07:00:22Z</updated><published>2020-09-28T07:00:22Z</published><content type="html">&lt;p&gt;In typical data warehousing systems, &lt;a href="https://developers.redhat.com/blog/category/big-data/"&gt;data&lt;/a&gt; is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from static sources (like databases) or real-time systems (like transactional applications), and then gets filtered, transformed, and finally stored in a database or pushed to several other systems for further processing. The other systems can then follow the same cycle—i.e., filter, transform, store, or push to other systems.&lt;/p&gt; &lt;p&gt;In this article, we will build a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; application that streams and processes data in real-time using &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt;. As we go through the example, you will learn how to apply &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka concepts&lt;/a&gt; such as joins, windows, processors, state stores, punctuators, and interactive queries. By the end of the article, you will have the architecture for a realistic data streaming pipeline in Quarkus.&lt;/p&gt; &lt;h2&gt;The traditional messaging system&lt;/h2&gt; &lt;p&gt;As developers, we are tasked with updating a message-processing system that was originally built using a relational database and a traditional message broker. Here&amp;#8217;s the data flow for the messaging system:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Data from two different systems arrives in two different messaging queues. Each record in one queue has a corresponding record in the other queue. Each record has a unique key.&lt;/li&gt; &lt;li&gt;When a data record arrives in one of the message queues, the system uses the record&amp;#8217;s unique key to determine whether the database already has an entry for that record. If it does not find a record with that unique key, the system inserts the record into the database for processing.&lt;/li&gt; &lt;li&gt;If the same data record arrives in the second queue within a few seconds, the application triggers the same logic. It checks whether a record with the same key is present in the database. If the record is present, the application retrieves the data and processes the two data objects.&lt;/li&gt; &lt;li&gt;If the data record doesn&amp;#8217;t arrive in the second queue within 50 seconds after arriving in the first queue, then another application processes the record in the database.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you might imagine, this scenario worked well before the advent of data streaming, but it does not work so well today.&lt;/p&gt; &lt;h2&gt;The data streaming pipeline&lt;/h2&gt; &lt;p&gt;Our task is to build a new message system that executes data streaming operations with Kafka. This type of application is capable of processing data in real-time, and it eliminates the need to maintain a database for unprocessed records. Figure 1 illustrates the data flow for the new application:&lt;/p&gt; &lt;div id="attachment_749567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution.jpg"&gt;&lt;img aria-describedby="caption-attachment-749567" class="wp-image-749567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg" alt="A flow diagram of the data-streaming pipeline's architecture." width="640" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-300x115.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-768x295.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749567" class="wp-caption-text"&gt;Figure 1: Architecture of the data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;In the next sections, we&amp;#8217;ll go through the process of building a data streaming pipeline with Kafka Streams in Quarkus. You can get the complete source code from the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;. Before we start coding the architecture, let&amp;#8217;s discuss joins and windows in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Joins and windows in Kafka Streams&lt;/h2&gt; &lt;p&gt;Kafka allows you to &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#joining"&gt;join&lt;/a&gt; records that arrive on two different topics. You are probably familiar with the concept of &lt;i&gt;joins&lt;/i&gt; in a relational database, where the data is static and available in two tables. In Kafka, joins work differently because the data is always streaming.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll look at the types of joins in a moment, but the first thing to note is that joins happen for data collected over a duration of time. Kafka calls this type of collection &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;windowing&lt;/a&gt;. Various &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;types of windows&lt;/a&gt; are available in Kafka. For our example, we will use a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing-tumbling"&gt;tumbling window&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Inner joins&lt;/h3&gt; &lt;p&gt;Now, let&amp;#8217;s consider how an inner join works. Assume that two separate data streams arrive in two different Kafka topics, which we will call the left and right topics. A record arriving in one topic has another relevant record (with the same key but a different value) that is also arriving in the other topic. The second record arrives after a brief time delay. As shown in Figure 2, we create a Kafka stream for each of the topics.&lt;/p&gt; &lt;div id="attachment_749587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749587" class="wp-image-749587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg" alt="A diagram of an inner join for two topics." width="640" height="211" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-300x99.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-768x253.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749587" class="wp-caption-text"&gt;Figure 2: Diagram of an inner join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The inner join on the left and right streams creates a new data stream. When it finds a matching record (with the same key) on both the left and right streams, Kafka emits a new record at time &lt;i&gt;t2&lt;/i&gt; in the new stream. Because the B record did not arrive on the right stream within the specified time window, Kafka Streams won&amp;#8217;t emit a new record for B.&lt;/p&gt; &lt;h3&gt;Outer joins&lt;/h3&gt; &lt;p&gt;Next, let&amp;#8217;s look at how an outer join works. Figure 3 shows the data flow for the outer join in our example:&lt;/p&gt; &lt;div id="attachment_749597" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749597" class="wp-image-749597 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg" alt="A diagram of an outer join." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-300x146.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-768x374.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg 1278w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749597" class="wp-caption-text"&gt;Figure 3: Diagram of an outer join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If we don&amp;#8217;t use the &amp;#8220;group by&amp;#8221; clause when we join two streams in Kafka Streams, then the join operation will emit three records. Streams in Kafka do not wait for the entire window; instead, they start emitting records whenever the condition for an outer join is true. So, when Record A on the left stream arrives at time &lt;i&gt;t1&lt;/i&gt;, the join operation immediately emits a new record. At time t2, the &lt;code&gt;outerjoin&lt;/code&gt; Kafka stream receives data from the right stream. The join operation immediately emits another record with the values from both the left and right records.&lt;/p&gt; &lt;p&gt;You would see different outputs if you used the &lt;code&gt;groupBy&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; functions on these Kafka streams. In that case, the streams would wait for the window to complete the duration, perform the join, and then emit the data, as previously shown in Figure 3.&lt;/p&gt; &lt;p&gt;Understanding how inner and outer joins work in Kafka Streams helps us find the best way to implement the data flow that we want. In this case, it is clear that we need to perform an outer join. This type of join allows us to retrieve records that appear in both the left and right topics, as well as records that appear in only one of them.&lt;/p&gt; &lt;p&gt;With that background out of the way, let&amp;#8217;s begin building our Kafka-based data streaming pipeline.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: We can use Quarkus extensions for Spring Web and Spring DI (dependency injection) to code in the &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; style using Spring-based annotations.&lt;/p&gt; &lt;h2&gt;Step 1: Perform the outer join&lt;/h2&gt; &lt;p&gt;To perform the outer join, we first create a class called &lt;code&gt;KafkaStreaming&lt;/code&gt;, then add the function &lt;code&gt;startStreamStreamOuterJoin()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;@RestController public class KafkaStreaming { private KafkaStreams streamsOuterJoin; private final String LEFT_STREAM_TOPIC = "left-stream-topic"; private final String RIGHT_STREAM_TOPIC = "right-stream-topic"; private final String OUTER_JOIN_STREAM_OUT_TOPIC = "stream-stream-outerjoin"; private final String PROCESSED_STREAM_OUT_TOPIC = "processed-topic"; private final String KAFKA_APP_ID = "outerjoin"; private final String KAFKA_SERVER_NAME = "localhost:9092"; @RequestMapping("/startstream/") public void startStreamStreamOuterJoin() { Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, KAFKA_APP_ID); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_SERVER_NAME); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final StreamsBuilder builder = new StreamsBuilder(); KStream&amp;#60;String, String&amp;#62; leftSource = builder.stream(LEFT_STREAM_TOPIC); KStream&amp;#60;String, String&amp;#62; rightSource = builder.stream(RIGHT_STREAM_TOPIC); // TODO 1 - Add state store // do the outer join // change the value to be a mix of both streams value // have a moving window of 5 seconds // output the last value received for a specific key during the window // push the data to OUTER_JOIN_STREAM_OUT_TOPIC topic leftSource.outerJoin(rightSource, (leftValue, rightValue) -&amp;#62; "left=" + leftValue + ", right=" + rightValue, JoinWindows.of(Duration.ofSeconds(5))) .groupByKey() .reduce(((key, lastValue) -&amp;#62; lastValue)) .toStream() .to(OUTER_JOIN_STREAM_OUT_TOPIC); // build the streams topology final Topology topology = builder.build(); // TODO - 2: Add processor code later streamsOuterJoin = new KafkaStreams(topology, props); streamsOuterJoin.start(); } } &lt;/pre&gt; &lt;p&gt;When we do a join, we create a new value that combines the data in the left and right topics. If any record with a key is missing in the left or right topic, then the new value will have the string &lt;code&gt;null&lt;/code&gt; as the value for the missing record. Also, the Kafka Stream &lt;code&gt;reduce&lt;/code&gt; function returns the last-aggregated value for all of the keys.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; and &lt;code&gt;TODO - 2:  Add processor code later&lt;/code&gt; comments are placeholders for code that we will add in the upcoming sections.&lt;/p&gt; &lt;h3&gt;The data flow so far&lt;/h3&gt; &lt;p&gt;Figure 4 illustrates the following data flow:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When a record with key A and value V1 comes into the left stream at time t1, Kafka Streams applies an outer join operation. At this point, the application creates a new record with key A and the value &lt;em&gt;left=V1, right=null&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When a record with key A and value V2 arrives in the right topic, Kafka Streams again applies an outer join operation. This creates a new record with key A and the value &lt;em&gt;left=V1, right=V2&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When the &lt;code&gt;reduce&lt;/code&gt; function is evaluated at the end of the duration window, the Kafka Streams API emits the last value that was computed, per the unique record key. In this case, it emits a record with key A and a value of &lt;em&gt;left=V1, right=V2&lt;/em&gt; into the new stream.&lt;/li&gt; &lt;li&gt;The new stream pushes the record to the &lt;code&gt;outerjoin&lt;/code&gt; topic.&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_786687" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-786687" class="wp-image-786687 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg" alt="A diagram of the data streaming pipeline." width="640" height="241" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-300x113.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-768x289.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-786687" class="wp-caption-text"&gt;Figure 4: The data streaming pipeline so far.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, we will add the state store and processor code.&lt;/p&gt; &lt;h2&gt;Step 2: Add the Kafka Streams processor&lt;/h2&gt; &lt;p&gt;We need to process the records that are being pushed to the &lt;code&gt;outerjoin&lt;/code&gt; topic by the outer join operation. Kafka Streams provides a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html"&gt;Processor API&lt;/a&gt; that we can use to write custom logic for record processing. To start, we define a custom processor, &lt;code&gt;DataProcessor&lt;/code&gt;, and add it to the streams topology in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;public class DataProcessor implements Processor&amp;#60;String, String&amp;#62;{ private ProcessorContext context; @Override public void init(ProcessorContext context) { this.context = context; } @Override public void process(String key, String value) { if(value.contains("null")) { // TODO 3: - let's process later } else { processRecord(key, value); //forward the processed data to processed-topic topic context.forward(key, value); } context.commit(); } @Override public void close() { } private void processRecord (String key, String value) { // your own custom logic. I just print System.out.println("==== Record Processed ==== key: "+key+" and value: "+value); } } &lt;/pre&gt; &lt;p&gt;The record is processed, and if the value does not contain a &lt;code&gt;null&lt;/code&gt; string, it is forwarded to the &lt;i&gt;sink&lt;/i&gt; topic (that is, the &lt;code&gt;processed-topic&lt;/code&gt; topic). In the bolded parts of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class below, we wire the topology to define the source topic (i.e., the &lt;code&gt;outerjoin&lt;/code&gt; topic), add the processor, and finally add a sink (i.e., the &lt;code&gt;processed-topic&lt;/code&gt; topic). Once it&amp;#8217;s done, we can add this piece of code to the &lt;code&gt;TODO - 2: Add processor code later&lt;/code&gt; section of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;// add another stream that reads data from OUTER_JOIN_STREAM_OUT_TOPIC topic topology.addSource("Source", OUTER_JOIN_STREAM_OUT_TOPIC); // add a processor to the stream so that each record is processed topology.addProcessor("StateProcessor", new ProcessorSupplier&amp;#60;String, String&amp;#62;() { public Processor&amp;#60;String, String&amp;#62; get() { return new DataProcessor(); }}, "Source"); topology.addSink("Sink", PROCESSED_STREAM_OUT_TOPIC, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;Note that all we do is to define the source topic (the &lt;code&gt;outerjoin&lt;/code&gt; topic), add an instance of our custom processor class, and then add the sink topic (the &lt;code&gt;processed-topic&lt;/code&gt; topic). The &lt;code&gt;context.forward()&lt;/code&gt; method in the custom processor sends the record to the sink topic.&lt;/p&gt; &lt;p&gt;Figure 5 shows the architecture that we have built so far.&lt;/p&gt; &lt;div id="attachment_786707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor.jpg"&gt;&lt;img aria-describedby="caption-attachment-786707" class="wp-image-786707 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg" alt="A diagram of the architecture in progress." width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-300x112.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-768x286.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786707" class="wp-caption-text"&gt;Figure 5: The architecture with the Kafka Streams processor added.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Step 3: Add the punctuator and StateStore&lt;/h2&gt; &lt;p&gt;If you looked closely at the &lt;code&gt;DataProcessor&lt;/code&gt; class, you probably noticed that we are only processing records that have both of the required (left-stream and right-stream) key values. We also need to process records that have just one of the values, but we want to introduce a delay before processing these records. In some cases, the other value will arrive in a later time window, and we don&amp;#8217;t want to process the records prematurely.&lt;/p&gt; &lt;h3&gt;State store&lt;/h3&gt; &lt;p&gt;In order to delay processing, we need to hold incoming records in a store of some kind, rather than an external database. Kafka Streams lets us store data in a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#state-stores"&gt;state store&lt;/a&gt;. We can use this type of store to hold recently received input records, track rolling aggregates, de-duplicate input records, and more.&lt;/p&gt; &lt;h3&gt;Punctuators&lt;/h3&gt; &lt;p&gt;Once we start holding records that have a missing value from either topic in a state store, we can use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#defining-a-stream-processor"&gt;punctuators&lt;/a&gt; to process them. As an example, we could add a &lt;code&gt;punctuator&lt;/code&gt; function to a &lt;code&gt;processorcontext.schedule()&lt;/code&gt; method. We can set the schedule to call the &lt;code&gt;punctuate()&lt;/code&gt; method.&lt;/p&gt; &lt;h3&gt;Add the state store&lt;/h3&gt; &lt;p&gt;Adding the following code to the &lt;code&gt;KafkaStreaming&lt;/code&gt; class adds a state store. Place this code where you see the &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; comment in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; // build the state store that will eventually store all unprocessed items Map&amp;#60;String, String&amp;#62; changelogConfig = newHashMap&amp;#60;&amp;#62;(); StoreBuilder&amp;#60;KeyValueStore&amp;#60;String, String&amp;#62;&amp;#62; stateStore = Stores.keyValueStoreBuilder( Stores.persistentKeyValueStore(STORE_NAME), Serdes.String(), Serdes.String()) .withLoggingEnabled(changelogConfig); ..... ..... ..... ..... // add the state store in the topology builder topology.addStateStore(stateStore, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;We have defined a state store that stores the key and value as a string. We&amp;#8217;ve also enabled logging, which is useful if the application dies and restarts. In that case, the state store won&amp;#8217;t lose data.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll modify the processor&amp;#8217;s &lt;code&gt;process()&lt;/code&gt; to put records with a missing value from either topic in the state store for later processing. Place the following code where you see the comment &lt;code&gt;TODO 3 - let's process later&lt;/code&gt; in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; if(value.contains("null")) { if (kvStore.get(key) != null) { // this means that the other value arrived first // you have both the values now and can process the record String newvalue = value.concat(" ").concat(kvStore.get(key)); process(key, newvalue); // remove the entry from the statestore (if any left or right record came first as an event) kvStore.delete(key); context.forward(key, newvalue); } else { // add to state store as either left or right data is missing System.out.println("Incomplete value: "+value+" detected. Putting into statestore for later processing"); kvStore.put(key, value); } } &lt;/pre&gt; &lt;h3&gt;Add the punctuator&lt;/h3&gt; &lt;p&gt;Next, we add the punctuator to the custom processor we&amp;#8217;ve just created. For this, we update the &lt;code&gt;DataProcessor&lt;/code&gt;&amp;#8216;s &lt;code&gt;init()&lt;/code&gt; method to the following:&lt;/p&gt; &lt;pre&gt; private KeyValueStore&amp;#60;String, String&amp;#62; kvStore; @Override public void init(ProcessorContext context) { this.context = context; kvStore = (KeyValueStore) context.getStateStore(STORE_NAME); // schedule a punctuate() method every 50 seconds based on stream-time this.context.schedule(Duration.ofSeconds(50), PunctuationType.WALL_CLOCK_TIME, new Punctuator(){ @Override public void punctuate(long timestamp) { System.out.println("Scheduled punctuator called at "+timestamp); KeyValueIterator&amp;#60;String, String&amp;#62; iter = kvStore.all(); while (iter.hasNext()) { KeyValue&amp;#60;String, String&amp;#62; entry = iter.next(); System.out.println(" Processed key: "+entry.key+" and value: "+entry.value+" and sending to processed-topic topic"); context.forward(entry.key, entry.value.toString()); kvStore.put(entry.key, null); } iter.close(); // commit the current processing progress context.commit(); } } ); } &lt;/pre&gt; &lt;p&gt;We&amp;#8217;ve set the punctuate logic to be invoked every 50 seconds. The code retrieves entries in the state store and processes them. The &lt;code&gt;forward()&lt;/code&gt; function then sends the processed record to the &lt;code&gt;processed-topic&lt;/code&gt; topic. Lastly, we delete the record from the state store.&lt;/p&gt; &lt;p&gt;Figure 6 shows the complete data streaming architecture:&lt;/p&gt; &lt;div id="attachment_786717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture.jpg"&gt;&lt;img aria-describedby="caption-attachment-786717" class="wp-image-786717 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg" alt="A diagram of the complete application with the state store and punctuators added." width="640" height="244" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-300x114.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-768x293.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786717" class="wp-caption-text"&gt;Figure 6: The complete data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Interactive queries&lt;/h2&gt; &lt;p&gt;We are finished with the basic data streaming pipeline, but what if we wanted to be able to query the state store? In this case, we could use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/interactive-queries.html"&gt;interactive queries&lt;/a&gt; in the Kafka Streams API to make the application queryable. See the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus/blob/master/quarkus-kafka-streaming/src/main/java/org/acme/InteractiveQueries.java"&gt;GitHub repository&lt;/a&gt; for more about interactive queries in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;You can use the streaming pipeline that we developed in this article to do any of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Process records in real-time.&lt;/li&gt; &lt;li&gt;Store data without depending on a database or cache.&lt;/li&gt; &lt;li&gt;Build a modern, &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope the example application and instructions will help you with building and processing data streaming pipelines. You can get the source code for the example application from this article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#038;title=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/" data-a2a-title="Build a data streaming pipeline using Kafka Streams and Quarkus"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/"&gt;Build a data streaming pipeline using Kafka Streams and Quarkus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6-PYLEG5zIc" height="1" width="1" alt=""/&gt;</content><summary>In typical data warehousing systems, data is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from static sources (like databases) or real-time systems (...</summary><dc:creator>Kapil Shukla</dc:creator><dc:date>2020-09-28T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/</feedburner:origLink></entry><entry><title>How to setup OpenShift Container Platform 4.5 on your local machine in minutes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zFfVGIyo8UA/how-to-setup-openshift-container-platform-45.html" /><category term="cloud" scheme="searchisko:content:tags" /><category term="CodeReadyContainers" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-how_to_setup_openshift_container_platform_4_5_on_your_local_machine_in_minutes</id><updated>2020-10-01T10:14:06Z</updated><published>2020-09-28T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s1600/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="CodeReady Containers" border="0" data-original-height="1200" data-original-width="1600" height="240" src="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s320/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform?&lt;br /&gt;&lt;br /&gt;Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5?&lt;br /&gt;&lt;br /&gt;Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud native development and automated rolling deployments. Since I started pulling together ways to easily experience this with OpenShift Container Platform, back with&amp;nbsp;&lt;a href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;version 3.3&lt;/a&gt;&amp;nbsp;believe it or not, we've come a long ways.&lt;br /&gt;&lt;br /&gt;The idea was to make this as streamlined of an experience as possible by using the same&amp;nbsp;&lt;a href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;CodeReady Containers Easy Install project&lt;/a&gt;. Let's take a look at what this looks like.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;Below is a walk through step by step, but first you can skip all that and just watch a video of how to install on your local machine:&lt;/div&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" height="381" src="https://www.youtube.com/embed/CJMdSQVFVik" width="641" youtube-src-id="CJMdSQVFVik"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Now let's take it step by step:&lt;br /&gt;&lt;h3&gt;Linux or Mac installation&lt;/h3&gt;This installation requires the following (all freely available):&lt;br /&gt;&lt;br /&gt;&lt;pre class="code highlight" lang="plaintext"&gt;&lt;span style="font-size: x-small;"&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span class="line" id="LC1" lang="plaintext"&gt;1. HyperKit for OSX, Hyper-V for Windows, or Libvirt for Linux&lt;/span&gt;&lt;br /&gt;&lt;span class="line" id="LC2" lang="plaintext"&gt;2. Code Ready Containers (OCP 4.5)&lt;/span&gt;&lt;br /&gt;&lt;span class="line" id="LC3" lang="plaintext"&gt;3. OpenShift Client (oc) v4.5&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;br /&gt;First you need to ensure your virtualization tooling is installed for your platform, just search online for how to do that or your specific platform. Second you need to download the CodeReady Containers. Finally, you need the OpenShift client. Normally you'd expect to have to track these last two down but we've made this all easy by just including checks during the installation. If you have something installed, it checks the version, if good then it moves on with next steps. If anything is missing or wrong version, the installation stops and notifies you where to find that component for your platform (including URL).&lt;br /&gt;&lt;br /&gt;Let's get started by downloading the&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/ocp-install-demo/-/archive/master/ocp-install-demo-master.zip" target="_blank"&gt;CodeReady Containers Easy Install&lt;/a&gt; project and unzipping in some directory. This gives you a file called&amp;nbsp;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span style="font-size: x-small;"&gt;ocp-install-demo-master.zip&lt;/span&gt;,&lt;/span&gt;just unzip and run the&amp;nbsp;&lt;span style="font-size: x-small;"&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.sh&lt;/span&gt;&lt;/span&gt;&amp;nbsp;as follows:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $ ./init.sh&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Follow the instructions as each of the dependencies is checked and you're provided with pointers to getting the versions you need for your platform.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Note: Each CodeReady Container download is tied to an embedded secret. This secret you need to download (link will be provided) as a file and you'll be asked to point to that secret to start your container platform.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Once you've gotten all the dependencies sorted out, the install runs like this:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="844" data-original-width="792" height="640" src="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" width="600" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;A little ASCII art and then it's checking for my platforms virtualization (Hyperkit), then looking for the OpenShift client version 4.5 (oc client), then running a setup (crc setup).&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="406" data-original-width="813" height="316" src="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;The next steps are providing the pull-secret-file, you can set this in the variables at the top of the installation script. Now the moment of truth, the CodeReady Containers cluster starts, which takes some time depending on your network (crc start). With a good networks it's about a five minute wait.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="578" data-original-width="978" height="378" src="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This is the logging you'll see as the OpenShift cluster starts on your local machine. The warning is normal, just some of the features have been trimmed to speed up deployment.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="259" data-original-width="780" height="212" src="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="461" data-original-width="553" height="531" src="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;At the end we'll retrieve the admin password for logging in to the cluster's console, pick up the host URL, test the deployment by logging in with our client (oc login), and finally you're given all the details in a nice box. You have the option to stop, start it again, or delete the OpenShift Container Platform cluster as shown in the dialog.&lt;br /&gt;&lt;br /&gt;Next open the web console using URL and login 'kubeadmin' with the corresponding password. In our case it's the URL:&amp;nbsp;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;https://console-openshift-console.apps-crc.testing&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s1600/ocp-login.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="804" data-original-width="1600" height="320" src="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s640/ocp-login.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Log in with user:&amp;nbsp;&lt;span style="font-size: x-small;"&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;kubeadmin&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;Password in our case:&amp;nbsp;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;duduw-yPT9Z-hsUpq-f3pre&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;That opens the main dashboard:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="898" data-original-width="1534" height="374" src="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Verify the version you are running by clicking on the top right question mark and then &lt;i&gt;About&lt;/i&gt;&amp;nbsp;option:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="900" data-original-width="1533" height="374" src="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Close the version window by clicking on the X. As we are interested in developing using the tooling and container images provided by CodeReady Containers, let's change the view from &lt;i&gt;Administrator &lt;/i&gt;to &lt;i&gt;Developer &lt;/i&gt;in the left top menu selecting &lt;i&gt;Topology &lt;/i&gt;and then via &lt;i&gt;Project&lt;/i&gt;&amp;nbsp;drop down menu at the top choose &lt;i&gt;Default&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="899" data-original-width="1537" height="374" src="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;You can browse the offerings in the provided container catalog by selecting &lt;i&gt;From Catalog&lt;/i&gt;&amp;nbsp;and then for example, &lt;i&gt;Middleware &lt;/i&gt;to view the offerings available:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="826" data-original-width="1536" height="344" src="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Looking to get started with an example usage, try the &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhpam-install-demo" target="_blank"&gt;Red Hat Process Automation Manager&lt;/a&gt; or &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhdm-install-demo" target="_blank"&gt;Red Hat Decision Manager &lt;/a&gt;examples that leverage the provided developer catalog container images. You can also explore how an existing project is setup using one of the developer catalog container images with a &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rewards-demo" target="_blank"&gt;human resources employee rewards project&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;This concludes the installation and tour of an OpenShift Container Platform on our local machine using CodeReady Containers.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;What about Windows?&lt;/h3&gt;If you are a sharp observer, you'll notice there is a file called&amp;nbsp;&lt;a href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.bat&lt;/span&gt;&lt;/a&gt;&amp;nbsp;for windows platforms to install with. The problem is I've not been able to test this yet on a windows machine, so I'd love to call out to the readers out there that might have some time to contribute to test this script and help us complete the installation. You'll notice a few TODO's marked in the scripts code, as they are untested areas in the installation.&lt;br /&gt;&lt;br /&gt;You can&amp;nbsp;&lt;a href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;raise new issues here&lt;/a&gt;&amp;nbsp;and help us complete the windows based installation and get your name added to the contributors list. We'd be really thankful!&lt;br /&gt;&lt;br /&gt;Stay tuned for more on cloud-native development using other Red Hat technologies on your new OpenShift Container Platform installed locally on your own machine!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/n_b1jiOlhGY" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zFfVGIyo8UA" height="1" width="1" alt=""/&gt;</content><summary>Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform? Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5? Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-09-28T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/n_b1jiOlhGY/how-to-setup-openshift-container-platform-45.html</feedburner:origLink></entry><entry><title>Rootless containers with Podman: The basics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qM0ia1DXS7w/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="linux" scheme="searchisko:content:tags" /><category term="oci image" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><category term="Podman" scheme="searchisko:content:tags" /><category term="RHEL 7" scheme="searchisko:content:tags" /><category term="rhel 8" scheme="searchisko:content:tags" /><category term="rootless containers" scheme="searchisko:content:tags" /><category term="rootless podman" scheme="searchisko:content:tags" /><category term="Universal Base Images (UBI)" scheme="searchisko:content:tags" /><author><name>Prakhar Sethi</name></author><id>searchisko:content:id:jbossorg_blog-rootless_containers_with_podman_the_basics</id><updated>2020-09-25T07:00:56Z</updated><published>2020-09-25T07:00:56Z</published><content type="html">&lt;p&gt;As a developer, you have probably heard a lot about containers. A &lt;a href="https://developers.redhat.com/topics/containers"&gt;&lt;i&gt;container&lt;/i&gt;&lt;/a&gt; is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools"&gt;Podman&lt;/a&gt;), which is a daemonless, open source, &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;-native tool that provides a command-line interface (CLI) similar to the docker container engine.&lt;/p&gt; &lt;p&gt;In this article, I will explain the benefits of using containers and Podman, introduce rootless containers and why they are important, and then show you how to use rootless containers with Podman with an example. Before we dive into the implementation, let&amp;#8217;s review the basics.&lt;/p&gt; &lt;p&gt;&lt;span id="more-748207"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Why containers?&lt;/h2&gt; &lt;p&gt;Using containers isolates your applications from the various computing environments in which they run. They have become increasingly popular because they help developers focus on the application logic and its dependencies, which they bind in a single unit. Operations teams also like containers because they can focus on managing the application, including deployment, without bothering with details such as software versions and configuration.&lt;/p&gt; &lt;p&gt;Containers virtualize at the operating system (OS) level. This makes them lightweight, unlike virtual machines, which virtualize at the hardware level. In a nutshell, here are the advantages of using containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low hardware footprint&lt;/li&gt; &lt;li&gt;Environment isolation&lt;/li&gt; &lt;li&gt;Quick deployment&lt;/li&gt; &lt;li&gt;Multiple environment deployments&lt;/li&gt; &lt;li&gt;Reusability&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why Podman?&lt;/h2&gt; &lt;p&gt;Using Podman makes it easy to find, run, build, share, and deploy applications using &lt;a target="_blank" rel="nofollow" href="https://opencontainers.org/"&gt;Open Container Initiative&lt;/a&gt; (OCI)-compatible containers and container images. Podman&amp;#8217;s advantages are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It is &lt;em&gt;daemonless&lt;/em&gt;; it does not require a daemon, unlike docker.&lt;/li&gt; &lt;li&gt;It lets you control the layers of the container; sometimes, you want a single layer, and sometimes you need 12 layers.&lt;/li&gt; &lt;li&gt;It uses the fork/exec model for containers instead of the client/server model.&lt;/li&gt; &lt;li&gt;It lets you run containers as a non-root user, so you never have to give a user root permission on the host. This obviously differs from the client/server model, where you must open a socket to a privileged daemon running as root to launch a container.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why rootless containers?&lt;/h2&gt; &lt;p&gt;&lt;i&gt;Rootless containers&lt;/i&gt; are containers that can be created, run, and managed by users without admin rights. Rootless containers have several advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They add a new security layer; even if the container engine, runtime, or orchestrator is compromised, the attacker won&amp;#8217;t gain root privileges on the host.&lt;/li&gt; &lt;li&gt;They allow multiple unprivileged users to run containers on the same machine (this is especially advantageous in &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/podman-paves-road-running-containerized-hpc-applications-exascale-supercomputers"&gt;high-performance computing environments&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;They allow for isolation inside of nested containers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To better understand these advantages, consider traditional resource management and scheduling systems. This type of system should be run by unprivileged users. From a security perspective, fewer privileges are better. With rootless containers, you can run a containerized process as any other process without needing to escalate any user&amp;#8217;s privileges. There is no daemon; Podman creates a child process.&lt;/p&gt; &lt;h2&gt;Example: Using rootless containers&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s get started using rootless containers with Podman. We&amp;#8217;ll start with the basic setup and configuration.&lt;/p&gt; &lt;h3&gt;System requirements&lt;/h3&gt; &lt;p&gt;We will need &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7.7 or greater for this implementation. Assuming you have that, we can begin configuring the example.&lt;/p&gt; &lt;h3&gt;Configuration&lt;/h3&gt; &lt;p&gt;First, install &lt;code&gt;slirp4netns&lt;/code&gt; and Podman on your machine by entering the following command:&lt;/p&gt; &lt;pre&gt;$ yum install slirp4netns podman -y &lt;/pre&gt; &lt;p&gt;We will use &lt;code&gt;slirp4netns&lt;/code&gt; to connect a network namespace to the internet in a completely rootless (or unprivileged) way.&lt;/p&gt; &lt;p&gt;When the installation is done, increase the number of user namespaces. Use the following commands :&lt;/p&gt; &lt;pre&gt;$ echo “user.max_user_namespaces=28633” &amp;#62; /etc/sysctl.d/userns.conf $ sysctl -p /etc/sysctl.d/userns.conf &lt;/pre&gt; &lt;p&gt;Next, create a new user account and name it. In this case, my user account is named Red Hat:&lt;/p&gt; &lt;pre&gt;$ useradd -c “Red Hat” redhat &lt;/pre&gt; &lt;p&gt;Use the following command to set the password for the new account (note that you must insert your own password):&lt;/p&gt; &lt;pre&gt;$ passwd redhat &lt;/pre&gt; &lt;p&gt;This user is now automatically configured to be able to use a rootless instance of Podman.&lt;/p&gt; &lt;h3&gt;Connect to the user&lt;/h3&gt; &lt;p&gt;Now, try running a Podman command as the user you&amp;#8217;ve just created.  Do not use &lt;code&gt;su -&lt;/code&gt; because that command doesn&amp;#8217;t set the correct environment variables. Instead, you can use any other command to connect to that user. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;$ ssh redhat@localhost &lt;/pre&gt; &lt;h3&gt;Pull a Red Hat Enterprise Linux image&lt;/h3&gt; &lt;p&gt;After logging in, try pulling a RHEL image using the &lt;code&gt;podman&lt;/code&gt; command (note that &lt;code&gt;ubi&lt;/code&gt; stands for &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;$ podman pull ubi7/ubi &lt;/pre&gt; &lt;p&gt;If you want more information about the image, run this command:&lt;/p&gt; &lt;pre&gt;$ podman run ubi7/ubi cat /etc/os-release &lt;/pre&gt; &lt;p&gt;To check the images that resulted from the above command, along with any other images on your system, run the command:&lt;/p&gt; &lt;pre&gt;$ podman images &lt;/pre&gt; &lt;p&gt;It is also possible for a rootless user to create a container from these images, but I&amp;#8217;ll save that for another article.&lt;/p&gt; &lt;h3&gt;Check the rootless configuration&lt;/h3&gt; &lt;p&gt;Finally, verify whether your rootless configuration is properly set up. Run the following command to show how the UIDs are assigned to the user namespace:&lt;/p&gt; &lt;pre&gt;$ podman unshare cat /proc/self/uid_map &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article demonstrated how to set up rootless containers with Podman. Here are some tips for working with rootless containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As a non-root container user, container images are stored under your home directory (for instance, &lt;code&gt;$HOME/.local/share/containers/storage&lt;/code&gt;), instead of &lt;code&gt;/var/lib/containers&lt;/code&gt;. This directory scheme ensures that you have enough storage for your home directory.&lt;/li&gt; &lt;li&gt;Users running rootless containers are given special permission to run on the host system using a range of user and group IDs. Otherwise, they have no root privileges to the operating system on the host.&lt;/li&gt; &lt;li&gt;A container running as root in a rootless account can turn on privileged features within its own namespace. But that doesn&amp;#8217;t provide any special privileges to access protected features on the host (beyond having extra UIDs and GIDs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Learn more about &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/index#set_up_for_rootless_containers"&gt;setting up rootless containers with Podman here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#038;title=Rootless%20containers%20with%20Podman%3A%20The%20basics" data-a2a-url="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/" data-a2a-title="Rootless containers with Podman: The basics"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/"&gt;Rootless containers with Podman: The basics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qM0ia1DXS7w" height="1" width="1" alt=""/&gt;</content><summary>As a developer, you have probably heard a lot about containers. A container is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (Podman), which is a daemonless, open source, Linux-native tool that provides a command-line inte...</summary><dc:creator>Prakhar Sethi</dc:creator><dc:date>2020-09-25T07:00:56Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/</feedburner:origLink></entry></feed>
